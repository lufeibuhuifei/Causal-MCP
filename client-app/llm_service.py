# client-app/llm_service.py
"""
LLMæœåŠ¡ç®¡ç†å™¨ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†ï¼ˆOllamaæœ¬åœ°æ¨¡å‹å’ŒDeepSeek APIï¼‰
"""

import logging
from typing import Optional, Dict, Any, List
import asyncio
import requests
import json
import re
import os
from pathlib import Path
from abc import ABC, abstractmethod
try:
    from .i18n import get_text
except ImportError:
    from i18n import get_text

logger = logging.getLogger(__name__)

class LLMProvider(ABC):
    """LLMæä¾›å•†æŠ½è±¡åŸºç±»"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_available = False

    @abstractmethod
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–LLMè¿æ¥"""
        pass

    @abstractmethod
    async def generate_text(self, prompt: str, max_length: int = 1024) -> Optional[str]:
        """ç”Ÿæˆæ–‡æœ¬"""
        pass

    @abstractmethod
    async def test_connection(self) -> tuple[bool, str, List[str]]:
        """æµ‹è¯•è¿æ¥"""
        pass

    @abstractmethod
    def get_provider_name(self) -> str:
        """è·å–æä¾›å•†åç§°"""
        pass

class OllamaProvider(LLMProvider):
    """Ollamaæœ¬åœ°LLMæä¾›å•†"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = config.get("base_url", "http://localhost:11434")
        self.model_name = config.get("model_name", "deepseek-r1:1.5b")
        self.temperature = config.get("temperature", 0.3)
        self.max_tokens = config.get("max_tokens", 2048)
        self.timeout = config.get("timeout", 60)

    def get_provider_name(self) -> str:
        return "Ollama"

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–Ollamaè¿æ¥"""
        try:
            logger.info(f"æ­£åœ¨åˆå§‹åŒ–OllamaæœåŠ¡ï¼Œæ¨¡å‹: {self.model_name}")

            # æµ‹è¯•Ollamaè¿æ¥
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code != 200:
                logger.warning(f"Ollama APIä¸å¯ç”¨: {response.status_code}")
                self.is_available = False
                return False

            models_data = response.json()
            available_models = [model['name'] for model in models_data.get('models', [])]

            if self.model_name not in available_models:
                logger.warning(f"æ¨¡å‹ {self.model_name} ä¸å¯ç”¨ã€‚å¯ç”¨æ¨¡å‹: {available_models}")
                self.is_available = False
                return False

            # æµ‹è¯•ç”Ÿæˆ
            test_payload = {
                "model": self.model_name,
                "prompt": "Hello",
                "stream": False,
                "options": {
                    "temperature": self.temperature,
                    "num_predict": 20
                }
            }

            response = requests.post(
                f"{self.base_url}/api/generate",
                json=test_payload,
                timeout=min(30, self.timeout)
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('response'):
                    self.is_available = True
                    logger.info("âœ… OllamaæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
                    return True

            logger.warning("âŒ Ollamaæµ‹è¯•ç”Ÿæˆå¤±è´¥")
            self.is_available = False
            return False

        except Exception as e:
            logger.warning(f"âš ï¸ Ollamaåˆå§‹åŒ–å¤±è´¥: {e}")
            self.is_available = False
            return False

    async def generate_text(self, prompt: str, max_length: int = 1024) -> Optional[str]:
        """ä½¿ç”¨Ollamaç”Ÿæˆæ–‡æœ¬"""
        if not self.is_available:
            return None

        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.temperature,
                    "num_predict": min(max_length, self.max_tokens)
                }
            }

            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=self.timeout
            )

            if response.status_code == 200:
                result = response.json()
                generated_text = result.get('response', '')
                if generated_text:
                    return generated_text.strip()

            logger.error(f"Ollama APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return None

        except Exception as e:
            logger.error(f"Ollamaæ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return None

    async def test_connection(self) -> tuple[bool, str, List[str]]:
        """æµ‹è¯•Ollamaè¿æ¥"""
        try:
            # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code != 200:
                return False, f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ (çŠ¶æ€ç : {response.status_code})", []

            models_data = response.json()
            available_models = [model['name'] for model in models_data.get('models', [])]

            if not available_models:
                return False, "OllamaæœåŠ¡è¿è¡Œæ­£å¸¸ï¼Œä½†æ²¡æœ‰å¯ç”¨æ¨¡å‹", []

            # æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å¯ç”¨
            if self.model_name not in available_models:
                return False, f"æŒ‡å®šæ¨¡å‹ {self.model_name} ä¸å¯ç”¨", available_models

            # æµ‹è¯•æ¨¡å‹ç”Ÿæˆ
            test_payload = {
                "model": self.model_name,
                "prompt": "Hello",
                "stream": False,
                "options": {
                    "temperature": self.temperature,
                    "num_predict": 20
                }
            }

            response = requests.post(
                f"{self.base_url}/api/generate",
                json=test_payload,
                timeout=min(30, self.timeout)
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('response'):
                    self.is_available = True
                    return True, "è¿æ¥æµ‹è¯•æˆåŠŸ", available_models
                else:
                    self.is_available = False
                    return False, "æ¨¡å‹å“åº”ä¸ºç©º", available_models
            else:
                self.is_available = False
                return False, f"æ¨¡å‹æµ‹è¯•å¤±è´¥ (çŠ¶æ€ç : {response.status_code})", available_models

        except requests.exceptions.Timeout:
            self.is_available = False
            return False, "è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ", []
        except requests.exceptions.ConnectionError:
            self.is_available = False
            return False, "æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·æ£€æŸ¥æœåŠ¡åœ°å€å’Œç«¯å£", []
        except Exception as e:
            self.is_available = False
            return False, f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}", []

class GeminiProvider(LLMProvider):
    """Google Gemini APIæä¾›å•†"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.api_key = config.get("api_key", "")
        self.base_url = config.get("base_url", "https://generativelanguage.googleapis.com")
        self.model_name = config.get("model_name", "gemini-1.5-flash")
        self.temperature = config.get("temperature", 0.3)
        self.max_tokens = config.get("max_tokens", 2048)
        self.timeout = config.get("timeout", 60)

    def get_provider_name(self) -> str:
        return "Gemini"

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–Gemini APIè¿æ¥"""
        try:
            logger.info(f"æ­£åœ¨åˆå§‹åŒ–Gemini APIæœåŠ¡ï¼Œæ¨¡å‹: {self.model_name}")

            if not self.api_key:
                logger.warning("Gemini APIå¯†é’¥æœªé…ç½®")
                self.is_available = False
                return False

            # æµ‹è¯•APIè¿æ¥
            success, message, _ = await self.test_connection()
            if success:
                self.is_available = True
                logger.info("âœ… Gemini APIæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                logger.warning(f"âŒ Gemini APIåˆå§‹åŒ–å¤±è´¥: {message}")
                self.is_available = False
                return False

        except Exception as e:
            logger.warning(f"âš ï¸ Gemini APIåˆå§‹åŒ–å¤±è´¥: {e}")
            self.is_available = False
            return False

    async def generate_text(self, prompt: str, max_length: int = 1024) -> Optional[str]:
        """ä½¿ç”¨Gemini APIç”Ÿæˆæ–‡æœ¬"""
        if not self.is_available or not self.api_key:
            return None

        try:
            headers = {
                "Content-Type": "application/json"
            }

            # Gemini APIä½¿ç”¨ä¸åŒçš„è¯·æ±‚æ ¼å¼
            payload = {
                "contents": [
                    {
                        "parts": [
                            {"text": prompt}
                        ]
                    }
                ],
                "generationConfig": {
                    "temperature": self.temperature,
                    "maxOutputTokens": min(max_length, self.max_tokens),
                    "candidateCount": 1
                }
            }

            # Gemini API URLæ ¼å¼
            url = f"{self.base_url}/v1beta/models/{self.model_name}:generateContent?key={self.api_key}"

            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=self.timeout
            )

            if response.status_code == 200:
                result = response.json()
                candidates = result.get('candidates', [])
                if candidates and len(candidates) > 0:
                    content = candidates[0].get('content', {})
                    parts = content.get('parts', [])
                    if parts and len(parts) > 0:
                        text = parts[0].get('text', '')
                        if text:
                            return text.strip()

            logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return None

        except Exception as e:
            logger.error(f"Gemini APIæ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return None

    async def test_connection(self) -> tuple[bool, str, List[str]]:
        """æµ‹è¯•Gemini APIè¿æ¥"""
        try:
            if not self.api_key:
                return False, "APIå¯†é’¥æœªé…ç½®", []

            headers = {
                "Content-Type": "application/json"
            }

            # ä½¿ç”¨ç®€å•çš„æµ‹è¯•è¯·æ±‚
            test_payload = {
                "contents": [
                    {
                        "parts": [
                            {"text": "Hello"}
                        ]
                    }
                ],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 10,
                    "candidateCount": 1
                }
            }

            url = f"{self.base_url}/v1beta/models/{self.model_name}:generateContent?key={self.api_key}"

            response = requests.post(
                url,
                headers=headers,
                json=test_payload,
                timeout=min(30, self.timeout)
            )

            if response.status_code == 200:
                result = response.json()
                candidates = result.get('candidates', [])
                if candidates and len(candidates) > 0:
                    self.is_available = True
                    # Geminiå¸¸ç”¨æ¨¡å‹åˆ—è¡¨
                    available_models = [
                        self.model_name,
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-1.0-pro"
                    ]
                    return True, "è¿æ¥æµ‹è¯•æˆåŠŸ", available_models
                else:
                    self.is_available = False
                    return False, "APIå“åº”æ ¼å¼å¼‚å¸¸", []
            elif response.status_code == 400:
                self.is_available = False
                error_detail = response.json().get('error', {}).get('message', response.text)
                return False, f"è¯·æ±‚æ ¼å¼é”™è¯¯: {error_detail}", []
            elif response.status_code == 403:
                self.is_available = False
                return False, "APIå¯†é’¥æ— æ•ˆæˆ–æƒé™ä¸è¶³", []
            elif response.status_code == 429:
                self.is_available = False
                return False, "APIè°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•", []
            else:
                self.is_available = False
                return False, f"APIè°ƒç”¨å¤±è´¥ (çŠ¶æ€ç : {response.status_code}): {response.text}", []

        except requests.exceptions.Timeout:
            self.is_available = False
            return False, "è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥", []
        except requests.exceptions.ConnectionError:
            self.is_available = False
            return False, "æ— æ³•è¿æ¥åˆ°Gemini APIæœåŠ¡", []
        except Exception as e:
            self.is_available = False
            return False, f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}", []

class DeepSeekProvider(LLMProvider):
    """DeepSeek APIæä¾›å•†"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.api_key = config.get("api_key", "")
        self.base_url = config.get("base_url", "https://api.deepseek.com")
        self.model_name = config.get("model_name", "deepseek-chat")
        self.temperature = config.get("temperature", 0.3)
        self.max_tokens = config.get("max_tokens", 2048)
        self.timeout = config.get("timeout", 60)

    def get_provider_name(self) -> str:
        return "DeepSeek"

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–DeepSeek APIè¿æ¥"""
        try:
            logger.info(f"æ­£åœ¨åˆå§‹åŒ–DeepSeek APIæœåŠ¡ï¼Œæ¨¡å‹: {self.model_name}")

            if not self.api_key:
                logger.warning("DeepSeek APIå¯†é’¥æœªé…ç½®")
                self.is_available = False
                return False

            # æµ‹è¯•APIè¿æ¥
            success, message, _ = await self.test_connection()
            if success:
                self.is_available = True
                logger.info("âœ… DeepSeek APIæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                logger.warning(f"âŒ DeepSeek APIåˆå§‹åŒ–å¤±è´¥: {message}")
                self.is_available = False
                return False

        except Exception as e:
            logger.warning(f"âš ï¸ DeepSeek APIåˆå§‹åŒ–å¤±è´¥: {e}")
            self.is_available = False
            return False

    async def generate_text(self, prompt: str, max_length: int = 1024) -> Optional[str]:
        """ä½¿ç”¨DeepSeek APIç”Ÿæˆæ–‡æœ¬"""
        if not self.is_available or not self.api_key:
            return None

        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }

            payload = {
                "model": self.model_name,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": self.temperature,
                "max_tokens": min(max_length, self.max_tokens),
                "stream": False
            }

            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=self.timeout
            )

            if response.status_code == 200:
                result = response.json()
                choices = result.get('choices', [])
                if choices and len(choices) > 0:
                    message = choices[0].get('message', {})
                    content = message.get('content', '')
                    if content:
                        return content.strip()

            logger.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return None

        except Exception as e:
            logger.error(f"DeepSeek APIæ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return None

    async def test_connection(self) -> tuple[bool, str, List[str]]:
        """æµ‹è¯•DeepSeek APIè¿æ¥"""
        try:
            if not self.api_key:
                return False, "APIå¯†é’¥æœªé…ç½®", []

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }

            # ä½¿ç”¨ç®€å•çš„æµ‹è¯•è¯·æ±‚
            test_payload = {
                "model": self.model_name,
                "messages": [
                    {"role": "user", "content": "Hello"}
                ],
                "temperature": 0.1,
                "max_tokens": 10,
                "stream": False
            }

            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=test_payload,
                timeout=min(30, self.timeout)
            )

            if response.status_code == 200:
                result = response.json()
                choices = result.get('choices', [])
                if choices and len(choices) > 0:
                    self.is_available = True
                    # DeepSeek APIä¸æä¾›æ¨¡å‹åˆ—è¡¨ï¼Œè¿”å›å½“å‰é…ç½®çš„æ¨¡å‹
                    available_models = [self.model_name, "deepseek-chat", "deepseek-reasoner"]
                    return True, "è¿æ¥æµ‹è¯•æˆåŠŸ", available_models
                else:
                    self.is_available = False
                    return False, "APIå“åº”æ ¼å¼å¼‚å¸¸", []
            elif response.status_code == 401:
                self.is_available = False
                return False, "APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ", []
            elif response.status_code == 429:
                self.is_available = False
                return False, "APIè°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•", []
            else:
                self.is_available = False
                return False, f"APIè°ƒç”¨å¤±è´¥ (çŠ¶æ€ç : {response.status_code}): {response.text}", []

        except requests.exceptions.Timeout:
            self.is_available = False
            return False, "è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥", []
        except requests.exceptions.ConnectionError:
            self.is_available = False
            return False, "æ— æ³•è¿æ¥åˆ°DeepSeek APIæœåŠ¡", []
        except Exception as e:
            self.is_available = False
            return False, f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}", []

class LLMConfig:
    """LLMé…ç½®ç®¡ç†ç±»"""

    def __init__(self):
        self.config_file = Path("llm_config.json")
        self.default_config = {
            "provider": "ollama",  # é»˜è®¤ä½¿ç”¨ollama
            "enabled": True,
            "ollama": {
                "base_url": "http://localhost:11434",
                "model_name": "deepseek-r1:1.5b",
                "temperature": 0.3,
                "max_tokens": 2048,
                "timeout": 60
            },
            "deepseek": {
                "api_key": "",
                "base_url": "https://api.deepseek.com",
                "model_name": "deepseek-chat",
                "temperature": 0.3,
                "max_tokens": 2048,
                "timeout": 60
            },
            "gemini": {
                "api_key": "",
                "base_url": "https://generativelanguage.googleapis.com",
                "model_name": "gemini-1.5-flash",
                "temperature": 0.3,
                "max_tokens": 2048,
                "timeout": 60
            }
        }

    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½LLMé…ç½®"""
        try:
            if self.config_file.exists():
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ—§æ ¼å¼é…ç½®ï¼Œå¦‚æœæ˜¯åˆ™è½¬æ¢ä¸ºæ–°æ ¼å¼
                    if "provider" not in config and "base_url" in config:
                        # æ—§æ ¼å¼è½¬æ¢ä¸ºæ–°æ ¼å¼
                        old_config = config.copy()
                        config = self.default_config.copy()
                        config["provider"] = "ollama"
                        config["ollama"].update({
                            "base_url": old_config.get("base_url", "http://localhost:11434"),
                            "model_name": old_config.get("model_name", "deepseek-r1:1.5b"),
                            "temperature": old_config.get("temperature", 0.3),
                            "max_tokens": old_config.get("max_tokens", 2048),
                            "timeout": old_config.get("timeout", 60)
                        })
                        config["enabled"] = old_config.get("enabled", True)
                        # ä¿å­˜è½¬æ¢åçš„é…ç½®
                        self.save_config(config)
                        logger.info("å·²å°†æ—§æ ¼å¼é…ç½®è½¬æ¢ä¸ºæ–°æ ¼å¼")
                    else:
                        # åˆå¹¶é»˜è®¤é…ç½®ï¼Œç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µå­˜åœ¨
                        merged_config = self.default_config.copy()
                        self._deep_update(merged_config, config)
                        config = merged_config
                    return config
        except Exception as e:
            logger.warning(f"åŠ è½½LLMé…ç½®å¤±è´¥: {e}")

        return self.default_config.copy()

    def _deep_update(self, base_dict: Dict[str, Any], update_dict: Dict[str, Any]):
        """æ·±åº¦æ›´æ–°å­—å…¸"""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_update(base_dict[key], value)
            else:
                base_dict[key] = value

    def save_config(self, config: Dict[str, Any]) -> bool:
        """ä¿å­˜LLMé…ç½®"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            logger.info(f"LLMé…ç½®å·²ä¿å­˜åˆ°: {self.config_file}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜LLMé…ç½®å¤±è´¥: {e}")
            return False

    def validate_config(self, config: Dict[str, Any]) -> tuple[bool, str]:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        # æ£€æŸ¥åŸºæœ¬å­—æ®µ
        if "provider" not in config:
            return False, "ç¼ºå°‘å¿…éœ€å­—æ®µ: provider"

        provider = config["provider"]
        if provider not in ["ollama", "deepseek", "gemini"]:
            return False, f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}"

        # æ£€æŸ¥æä¾›å•†ç‰¹å®šé…ç½®
        if provider not in config:
            return False, f"ç¼ºå°‘æä¾›å•†é…ç½®: {provider}"

        provider_config = config[provider]

        if provider == "ollama":
            required_fields = ["base_url", "model_name"]
            for field in required_fields:
                if field not in provider_config or not provider_config[field]:
                    return False, f"Ollamaé…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
        elif provider == "deepseek":
            required_fields = ["api_key", "model_name"]
            for field in required_fields:
                if field not in provider_config or not provider_config[field]:
                    return False, f"DeepSeeké…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
        elif provider == "gemini":
            required_fields = ["api_key", "model_name"]
            for field in required_fields:
                if field not in provider_config or not provider_config[field]:
                    return False, f"Geminié…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"

        # éªŒè¯URLæ ¼å¼
        base_url = provider_config.get("base_url", "")
        if base_url and not base_url.startswith(("http://", "https://")):
            return False, "æœåŠ¡åœ°å€å¿…é¡»ä»¥ http:// æˆ– https:// å¼€å¤´"

        # éªŒè¯æ•°å€¼å‚æ•°
        try:
            if "temperature" in provider_config:
                temp = float(provider_config["temperature"])
                if not 0 <= temp <= 2:
                    return False, "æ¸©åº¦å‚æ•°å¿…é¡»åœ¨ 0-2 ä¹‹é—´"

            if "max_tokens" in provider_config:
                max_tokens = int(provider_config["max_tokens"])
                if max_tokens <= 0:
                    return False, "æœ€å¤§ä»¤ç‰Œæ•°å¿…é¡»å¤§äº 0"

            if "timeout" in provider_config:
                timeout = int(provider_config["timeout"])
                if timeout <= 0:
                    return False, "è¶…æ—¶æ—¶é—´å¿…é¡»å¤§äº 0"

        except (ValueError, TypeError):
            return False, "æ•°å€¼å‚æ•°æ ¼å¼é”™è¯¯"

        return True, "é…ç½®éªŒè¯é€šè¿‡"

class LLMService:
    """
    LLMæœåŠ¡ç®¡ç†å™¨ï¼Œæä¾›æ™ºèƒ½åˆ†æå’Œæ–‡æœ¬ç”ŸæˆåŠŸèƒ½
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–LLMæœåŠ¡

        Args:
            config: LLMé…ç½®å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä»é…ç½®æ–‡ä»¶åŠ è½½
        """
        self.config_manager = LLMConfig()

        if config is None:
            self.config = self.config_manager.load_config()
        else:
            self.config = config

        self.enabled = self.config.get("enabled", True)
        self.provider_name = self.config.get("provider", "ollama")

        # åˆ›å»ºå¯¹åº”çš„æä¾›å•†å®ä¾‹
        self.provider = self._create_provider()
        self.is_available = False

    def _create_provider(self) -> LLMProvider:
        """åˆ›å»ºLLMæä¾›å•†å®ä¾‹"""
        if self.provider_name == "ollama":
            provider_config = self.config.get("ollama", {})
            return OllamaProvider(provider_config)
        elif self.provider_name == "deepseek":
            provider_config = self.config.get("deepseek", {})
            return DeepSeekProvider(provider_config)
        elif self.provider_name == "gemini":
            provider_config = self.config.get("gemini", {})
            return GeminiProvider(provider_config)
        else:
            # é»˜è®¤ä½¿ç”¨Ollama
            logger.warning(f"æœªçŸ¥çš„æä¾›å•†: {self.provider_name}ï¼Œä½¿ç”¨é»˜è®¤Ollama")
            provider_config = self.config.get("ollama", {})
            return OllamaProvider(provider_config)
        
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–LLMè¿æ¥

        Returns:
            bool: æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨LLMæœåŠ¡
        if not self.enabled:
            logger.info("LLMæœåŠ¡å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨åŸºäºè§„åˆ™çš„æ™ºèƒ½ç”Ÿæˆ")
            self.is_available = False
            return True

        try:
            # ä½¿ç”¨æä¾›å•†è¿›è¡Œåˆå§‹åŒ–
            success = await self.provider.initialize()
            if success:
                self.is_available = True
                logger.info(f"âœ… {self.provider.get_provider_name()} LLMæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                logger.info("å°†ä½¿ç”¨å¢å¼ºçš„åŸºäºè§„åˆ™çš„æ™ºèƒ½ç”Ÿæˆ")
                self.is_available = False
                return True  # ä»ç„¶è¿”å›Trueï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•

        except Exception as e:
            logger.warning(f"âš ï¸ LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            logger.info("å°†ä½¿ç”¨å¢å¼ºçš„åŸºäºè§„åˆ™çš„æ™ºèƒ½ç”Ÿæˆ")
            self.is_available = False
            return True  # ä»ç„¶è¿”å›Trueï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•

    async def _generate_text(self, prompt: str, max_length: int = 1024) -> Optional[str]:
        """
        ç”Ÿæˆæ–‡æœ¬çš„åŸºç¡€æ–¹æ³•

        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§è¾“å‡ºé•¿åº¦

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬æˆ–None
        """
        if not self.is_available:
            logger.warning("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
            return None

        try:
            # ä½¿ç”¨æä¾›å•†ç”Ÿæˆæ–‡æœ¬
            generated_text = await self.provider.generate_text(prompt, max_length)
            if generated_text:
                # æ¸…ç†æ€è€ƒè¿‡ç¨‹æ ‡ç­¾
                cleaned_text = self._clean_thinking_process(generated_text)
                return cleaned_text.strip()
            return None

        except Exception as e:
            logger.error(f"LLMæ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def _clean_thinking_process(self, text: str, show_thinking: bool = False) -> str:
        """
        æ¸…ç†LLMè¾“å‡ºä¸­çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾

        Args:
            text: åŸå§‹LLMè¾“å‡º
            show_thinking: æ˜¯å¦ä¿ç•™æ€è€ƒè¿‡ç¨‹

        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return text

        if not show_thinking:
            # ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹
            import re
            # åŒ¹é… <think> åˆ° </think> ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬æ¢è¡Œï¼‰
            pattern = r'<think>.*?</think>'
            cleaned = re.sub(pattern, '', text, flags=re.DOTALL)

            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            lines = cleaned.split('\n')
            cleaned_lines = []
            prev_empty = False

            for line in lines:
                line = line.strip()
                if line:
                    cleaned_lines.append(line)
                    prev_empty = False
                elif not prev_empty:
                    cleaned_lines.append('')
                    prev_empty = True

            # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºè¡Œ
            while cleaned_lines and not cleaned_lines[0]:
                cleaned_lines.pop(0)
            while cleaned_lines and not cleaned_lines[-1]:
                cleaned_lines.pop()

            return '\n'.join(cleaned_lines)
        else:
            # ä¿ç•™æ€è€ƒè¿‡ç¨‹ï¼Œä½†ç”¨ç‰¹æ®Šæ ¼å¼æ ‡è®°
            import re
            def replace_think_tags(match):
                content = match.group(1)
                return f"\nğŸ’­ **æ€è€ƒè¿‡ç¨‹**:\n```\n{content.strip()}\n```\n"

            pattern = r'<think>(.*?)</think>'
            return re.sub(pattern, replace_think_tags, text, flags=re.DOTALL)
    
    async def generate_analysis_interpretation(
        self,
        exposure_gene: str,
        outcome_trait: str,
        mr_results: Dict[str, Any],
        language: str = "en",
        show_thinking: bool = False
    ) -> str:
        """
        ç”ŸæˆMRåˆ†æç»“æœçš„æ™ºèƒ½è§£é‡Š
        
        Args:
            exposure_gene: æš´éœ²åŸºå› 
            outcome_trait: ç»“å±€æ€§çŠ¶
            mr_results: MRåˆ†æç»“æœ
            language: è¯­è¨€ ("zh" æˆ– "en")
            
        Returns:
            æ™ºèƒ½ç”Ÿæˆçš„åˆ†æè§£é‡Š
        """
        
        # æ„å»ºæç¤ºæ¨¡æ¿
        if language == "zh":
            prompt_template = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç”Ÿç‰©ç»Ÿè®¡å­¦å®¶å’Œé—ä¼ æµè¡Œç—…å­¦ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹å­Ÿå¾·å°”éšæœºåŒ–(MR)åˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šä¸”æ˜“æ‡‚çš„åˆ†ææŠ¥å‘Šã€‚

## åˆ†æä¿¡æ¯
- æš´éœ²å˜é‡: {exposure_gene} åŸºå› è¡¨è¾¾
- ç»“å±€å˜é‡: {outcome_trait}
- åˆ†ææ–¹æ³•: å­Ÿå¾·å°”éšæœºåŒ–

## MRåˆ†æç»“æœ
{mr_results_summary}

## è¯·æä¾›ä»¥ä¸‹å†…å®¹ï¼š

### 1. ä¸»è¦å‘ç°
ç®€æ˜æ‰¼è¦åœ°æ€»ç»“å› æœå…³ç³»çš„ä¸»è¦å‘ç°ã€‚

### 2. ç»Ÿè®¡å­¦è§£é‡Š
è§£é‡Šå„ç§MRæ–¹æ³•çš„ç»“æœåŠå…¶ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚

### 3. ç”Ÿç‰©å­¦æ„ä¹‰
ä»ç”Ÿç‰©å­¦è§’åº¦è§£é‡Šè¿™ç§å› æœå…³ç³»çš„å¯èƒ½æœºåˆ¶ã€‚

### 4. ä¸´åºŠæ„ä¹‰
è®¨è®ºè¿™ä¸€å‘ç°å¯¹ä¸´åºŠå®è·µå’Œå…¬å…±å«ç”Ÿçš„æ½œåœ¨å½±å“ã€‚

### 5. ç ”ç©¶å±€é™æ€§
æŒ‡å‡ºåˆ†æçš„å±€é™æ€§å’Œéœ€è¦æ³¨æ„çš„é—®é¢˜ã€‚

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€æ’°å†™ï¼Œé€‚åˆç§‘ç ”äººå‘˜å’Œä¸´åºŠåŒ»ç”Ÿé˜…è¯»ã€‚
"""
        else:
            prompt_template = """
You are a professional biostatistician and genetic epidemiologist. Please generate a professional and accessible analysis report based on the following Mendelian Randomization (MR) analysis results.

## Analysis Information
- Exposure: {exposure_gene} gene expression
- Outcome: {outcome_trait}
- Method: Mendelian Randomization

## MR Analysis Results
{mr_results_summary}

## Please provide the following content:

### 1. Key Findings
Summarize the main findings regarding the causal relationship.

### 2. Statistical Interpretation
Explain the results from different MR methods and their statistical significance.

### 3. Biological Significance
Interpret the potential biological mechanisms underlying this causal relationship.

### 4. Clinical Implications
Discuss the potential impact of these findings on clinical practice and public health.

### 5. Study Limitations
Point out the limitations of the analysis and important considerations.

Please write in professional but accessible language, suitable for researchers and clinicians.
"""
        
        # å‡†å¤‡MRç»“æœæ‘˜è¦
        mr_summary = self._format_mr_results(mr_results, language)
        
        # æ„å»ºå®Œæ•´æç¤º
        prompt = prompt_template.format(
            exposure_gene=exposure_gene,
            outcome_trait=outcome_trait,
            mr_results_summary=mr_summary
        )
        
        # ç”Ÿæˆè§£é‡Š
        interpretation = await self._generate_text(prompt, max_length=2048)

        if interpretation:
            # æ¸…ç†æ€è€ƒè¿‡ç¨‹
            cleaned_interpretation = self._clean_thinking_process(interpretation, show_thinking)
            return cleaned_interpretation
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºè§„åˆ™çš„è§£é‡Š
            return self._generate_fallback_interpretation(exposure_gene, outcome_trait, mr_results, language)
    
    def _format_mr_results(self, mr_results: Dict[str, Any], language: str = "en") -> str:
        """
        æ ¼å¼åŒ–MRç»“æœä¸ºæ–‡æœ¬æ‘˜è¦
        """
        if not mr_results or "results" not in mr_results:
            return "æ— å¯ç”¨çš„MRåˆ†æç»“æœ" if language == "zh" else "No MR analysis results available"
        
        summary_parts = []
        
        for result in mr_results["results"]:
            method = result.get("method", "Unknown")
            estimate = result.get("estimate", 0)
            p_value = result.get("p_value", 1)
            ci_lower = result.get("ci_lower", 0)
            ci_upper = result.get("ci_upper", 0)
            
            if language == "zh":
                summary_parts.append(
                    f"- {method}: å› æœæ•ˆåº” = {estimate:.3f} "
                    f"(95% CI: {ci_lower:.3f} to {ci_upper:.3f}), P = {p_value:.2e}"
                )
            else:
                summary_parts.append(
                    f"- {method}: Causal effect = {estimate:.3f} "
                    f"(95% CI: {ci_lower:.3f} to {ci_upper:.3f}), P = {p_value:.2e}"
                )
        
        return "\n".join(summary_parts)
    
    def _generate_fallback_interpretation(
        self,
        exposure_gene: str,
        outcome_trait: str,
        mr_results: Dict[str, Any],
        language: str = "en"
    ) -> str:
        """
        å¢å¼ºçš„åŸºäºè§„åˆ™çš„è§£é‡Šç”Ÿæˆ
        """
        # åˆ†æMRç»“æœ
        mr_summary = self._analyze_mr_results(mr_results)

        # ä½¿ç”¨å›½é™…åŒ–æ–‡æœ¬æ„å»ºæŠ¥å‘Š
        report_parts = [
            f"## {get_text('mr_analysis_report', language)}",
            "",
            f"### {get_text('key_findings', language)}",
            get_text('mr_study_description', language).format(exposure_gene, outcome_trait),
            "",
            mr_summary[language],
            "",
            f"### {get_text('methodological_advantages', language)}",
            f"- {get_text('genetic_variants_iv', language)}",
            f"- {get_text('multiple_mr_validation', language)}",
            f"- {get_text('sensitivity_analysis', language)}",
            "",
            f"### {get_text('biological_significance', language)}",
            get_text('expression_mechanisms', language).format(exposure_gene, outcome_trait),
            get_text('direct_regulation', language),
            get_text('protein_function', language),
            get_text('gene_networks', language),
            "",
            f"### {get_text('clinical_translation', language)}",
            get_text('drug_target_identification', language).format(exposure_gene, outcome_trait),
            get_text('personalized_medicine', language),
            get_text('prevention_strategies', language),
            "",
            f"### {get_text('study_limitations', language)}",
            get_text('population_specific', language),
            get_text('larger_samples', language),
            get_text('functional_validation', language),
            "",
            f"### {get_text('future_research', language)}",
            get_text('validate_cohorts', language),
            get_text('functional_experiments', language),
            get_text('clinical_feasibility', language),
            get_text('drug_validation', language),
            "",
            get_text('enhanced_system_note', language)
        ]

        return "\n".join(report_parts)

    def _analyze_mr_results(self, mr_results: Dict[str, Any]) -> Dict[str, str]:
        """åˆ†æMRç»“æœå¹¶ç”Ÿæˆæ™ºèƒ½æ€»ç»“"""
        if not mr_results or "results" not in mr_results:
            return {
                "zh": get_text("no_mr_results", "zh"),
                "en": get_text("no_mr_results", "en")
            }

        results = mr_results["results"]

        # æ‰¾åˆ°IVWç»“æœ
        ivw_result = None
        for result in results:
            if "Inverse Variance" in result.get("method", ""):
                ivw_result = result
                break

        if not ivw_result:
            ivw_result = results[0] if results else None

        if not ivw_result:
            return {
                "zh": get_text("no_valid_results", "zh"),
                "en": get_text("no_valid_results", "en")
            }

        estimate = ivw_result.get("estimate", 0)
        p_value = ivw_result.get("p_value", 1)
        ci_lower = ivw_result.get("ci_lower", 0)
        ci_upper = ivw_result.get("ci_upper", 0)

        # åˆ¤æ–­æ˜¾è‘—æ€§å’Œæ•ˆåº”æ–¹å‘
        is_significant = p_value < 0.05
        effect_direction = "æ­£å‘" if estimate > 0 else "è´Ÿå‘"
        effect_direction_en = "positive" if estimate > 0 else "negative"

        if is_significant:
            zh_parts = [
                get_text("significant_causal_effect", "zh"),
                get_text("causal_effect_estimate", "zh").format(estimate, ci_lower, ci_upper),
                get_text("statistical_significance", "zh").format(p_value),
                get_text("effect_assessment_significant", "zh"),
                "",
                get_text("conclusion_significant", "zh")
            ]

            en_parts = [
                get_text("significant_causal_effect", "en"),
                get_text("causal_effect_estimate", "en").format(estimate, ci_lower, ci_upper),
                get_text("statistical_significance", "en").format(p_value),
                get_text("effect_assessment_significant", "en"),
                "",
                get_text("conclusion_significant", "en")
            ]
        else:
            zh_parts = [
                get_text("no_significant_effect_detected", "zh"),
                get_text("causal_effect_estimate", "zh").format(estimate, ci_lower, ci_upper),
                get_text("statistical_significance", "zh").format(p_value),
                get_text("effect_assessment_nonsignificant", "zh"),
                "",
                get_text("conclusion_nonsignificant", "zh")
            ]

            en_parts = [
                get_text("no_significant_effect_detected", "en"),
                get_text("causal_effect_estimate", "en").format(estimate, ci_lower, ci_upper),
                get_text("statistical_significance", "en").format(p_value),
                get_text("effect_assessment_nonsignificant", "en"),
                "",
                get_text("conclusion_nonsignificant", "en")
            ]

        return {
            "zh": "\n".join(zh_parts),
            "en": "\n".join(en_parts)
        }

    def _format_gene_data(self, gene_info: Dict[str, Any], language: str = "en") -> str:
        """
        æ ¼å¼åŒ–åŸºå› æ•°æ®ä¸ºæ–‡æœ¬æ‘˜è¦
        """
        try:
            summary_parts = []

            if language == "zh":
                if gene_info.get("protein_name"):
                    summary_parts.append(f"è›‹ç™½è´¨åç§°: {gene_info['protein_name']}")

                if gene_info.get("function"):
                    function_text = gene_info["function"]
                    if len(function_text) > 300:
                        function_text = function_text[:300] + "..."
                    summary_parts.append(f"åˆ†å­åŠŸèƒ½: {function_text}")

                if gene_info.get("subcellular_location"):
                    summary_parts.append(f"äºšç»†èƒå®šä½: {gene_info['subcellular_location']}")

                if gene_info.get("description"):
                    desc_text = gene_info["description"]
                    if len(desc_text) > 200:
                        desc_text = desc_text[:200] + "..."
                    summary_parts.append(f"åŸºå› æè¿°: {desc_text}")

            else:  # English
                if gene_info.get("protein_name"):
                    summary_parts.append(f"Protein name: {gene_info['protein_name']}")

                if gene_info.get("function"):
                    function_text = gene_info["function"]
                    if len(function_text) > 300:
                        function_text = function_text[:300] + "..."
                    summary_parts.append(f"Molecular function: {function_text}")

                if gene_info.get("subcellular_location"):
                    summary_parts.append(f"Subcellular location: {gene_info['subcellular_location']}")

                if gene_info.get("description"):
                    desc_text = gene_info["description"]
                    if len(desc_text) > 200:
                        desc_text = desc_text[:200] + "..."
                    summary_parts.append(f"Gene description: {desc_text}")

            return "\n".join(summary_parts) if summary_parts else "No detailed information available"

        except Exception as e:
            logger.error(f"åŸºå› æ•°æ®æ ¼å¼åŒ–å¤±è´¥: {e}")
            return "Data formatting error"

    def _format_drug_data(self, drug_targets: List[Dict[str, Any]], language: str = "en") -> str:
        """
        æ ¼å¼åŒ–è¯ç‰©æ•°æ®ä¸ºæ–‡æœ¬æ‘˜è¦
        """
        try:
            if not drug_targets:
                if language == "zh":
                    return "ç›®å‰æ²¡æœ‰å·²çŸ¥è¯ç‰©ç›´æ¥é¶å‘è¯¥åŸºå› "
                else:
                    return "No known drugs currently target this gene"

            summary_parts = []

            if language == "zh":
                summary_parts.append(f"å‘ç° {len(drug_targets)} ä¸ªç›¸å…³è¯ç‰©é¶ç‚¹:")

                for i, drug in enumerate(drug_targets[:5]):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    drug_name = drug.get("drug_name", drug.get("name", f"è¯ç‰© {i+1}"))
                    drug_type = drug.get("drug_type", drug.get("type", ""))
                    mechanism = drug.get("mechanism", drug.get("action_type", ""))

                    drug_info = f"- {drug_name}"
                    if drug_type:
                        drug_info += f" ({drug_type})"
                    if mechanism:
                        drug_info += f" - {mechanism}"

                    summary_parts.append(drug_info)

                if len(drug_targets) > 5:
                    summary_parts.append(f"- ä»¥åŠå…¶ä»– {len(drug_targets) - 5} ä¸ªè¯ç‰©...")

            else:  # English
                summary_parts.append(f"Found {len(drug_targets)} related drug targets:")

                for i, drug in enumerate(drug_targets[:5]):
                    drug_name = drug.get("drug_name", drug.get("name", f"Drug {i+1}"))
                    drug_type = drug.get("drug_type", drug.get("type", ""))
                    mechanism = drug.get("mechanism", drug.get("action_type", ""))

                    drug_info = f"- {drug_name}"
                    if drug_type:
                        drug_info += f" ({drug_type})"
                    if mechanism:
                        drug_info += f" - {mechanism}"

                    summary_parts.append(drug_info)

                if len(drug_targets) > 5:
                    summary_parts.append(f"- And {len(drug_targets) - 5} other drugs...")

            return "\n".join(summary_parts)

        except Exception as e:
            logger.error(f"è¯ç‰©æ•°æ®æ ¼å¼åŒ–å¤±è´¥: {e}")
            if language == "zh":
                return "æ•°æ®æ ¼å¼åŒ–é”™è¯¯"
            else:
                return "Data formatting error"

    async def generate_recommendations(
        self,
        analysis_result: Dict[str, Any],
        language: str = "en",
        show_thinking: bool = False
    ) -> List[str]:
        """
        ç”Ÿæˆæ™ºèƒ½åŒ–çš„ç ”ç©¶å»ºè®®
        """
        if language == "zh":
            prompt = f"""
åŸºäºä»¥ä¸‹å­Ÿå¾·å°”éšæœºåŒ–åˆ†æç»“æœï¼Œè¯·ç”Ÿæˆ5-7æ¡å…·ä½“çš„åç»­ç ”ç©¶å»ºè®®ï¼š

åˆ†æç»“æœæ‘˜è¦ï¼š
{analysis_result.get('summary', {})}

è¯·æä¾›å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®ï¼ŒåŒ…æ‹¬ï¼š
1. éªŒè¯å®éªŒè®¾è®¡
2. æ‰©å±•ç ”ç©¶æ–¹å‘
3. ä¸´åºŠè½¬åŒ–å»ºè®®
4. æ–¹æ³•å­¦æ”¹è¿›
5. æ•°æ®æ”¶é›†å»ºè®®

æ¯æ¡å»ºè®®åº”è¯¥ç®€æ´æ˜ç¡®ï¼Œé€‚åˆç ”ç©¶äººå‘˜å‚è€ƒã€‚
"""
        else:
            prompt = f"""
Based on the following Mendelian randomization analysis results, please generate 5-7 specific follow-up research recommendations:

Analysis summary:
{analysis_result.get('summary', {})}

Please provide specific, actionable recommendations including:
1. Validation experiment design
2. Extended research directions
3. Clinical translation suggestions
4. Methodological improvements
5. Data collection recommendations

Each recommendation should be concise and clear, suitable for researchers' reference.
"""
        
        recommendations_text = await self._generate_text(prompt, max_length=1024)

        if recommendations_text:
            # æ¸…ç†æ€è€ƒè¿‡ç¨‹
            cleaned_text = self._clean_thinking_process(recommendations_text, show_thinking)

            # è§£æç”Ÿæˆçš„å»ºè®®
            recommendations = []
            lines = cleaned_text.split('\n')
            for line in lines:
                line = line.strip()
                if line and (line.startswith('-') or line.startswith('â€¢') or re.match(r'^\d+\.', line)):
                    # æ¸…ç†æ ¼å¼
                    clean_line = re.sub(r'^[-â€¢\d\.]\s*', '', line)
                    if clean_line:
                        recommendations.append(clean_line)
            
            return recommendations[:7]  # æœ€å¤šè¿”å›7æ¡å»ºè®®
        else:
            # å¤‡ç”¨å»ºè®®
            if language == "zh":
                return [
                    "åœ¨ç‹¬ç«‹é˜Ÿåˆ—ä¸­éªŒè¯å› æœå…³ç³»",
                    "è¿›è¡ŒåŠŸèƒ½å®éªŒéªŒè¯åˆ†å­æœºåˆ¶",
                    "è¯„ä¼°ä¸´åºŠåº”ç”¨çš„å¯è¡Œæ€§",
                    "æ‰©å¤§æ ·æœ¬é‡æé«˜ç»Ÿè®¡åŠŸæ•ˆ",
                    "è€ƒè™‘äººç¾¤åˆ†å±‚åˆ†æ"
                ]
            else:
                return [
                    "Validate causal relationship in independent cohorts",
                    "Conduct functional experiments to verify molecular mechanisms",
                    "Assess feasibility of clinical applications",
                    "Increase sample size to improve statistical power",
                    "Consider population stratification analysis"
                ]

    async def generate_gene_interpretation(
        self,
        gene_symbol: str,
        gene_info: Dict[str, Any],
        language: str = "en",
        show_thinking: bool = False
    ) -> str:
        """
        ç”ŸæˆåŸºå› åŠŸèƒ½çš„æ™ºèƒ½è§£é‡Š

        Args:
            gene_symbol: åŸºå› ç¬¦å·
            gene_info: åŸºå› ä¿¡æ¯æ•°æ®
            language: è¯­è¨€ ("zh" æˆ– "en")
            show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹

        Returns:
            æ™ºèƒ½ç”Ÿæˆçš„åŸºå› åŠŸèƒ½è§£é‡Š
        """

        # æ„å»ºæç¤ºæ¨¡æ¿
        if language == "zh":
            prompt_template = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åˆ†å­ç”Ÿç‰©å­¦å®¶å’ŒåŸºå› ç»„å­¦ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹åŸºå› ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šä¸”æ˜“æ‡‚çš„åŸºå› åŠŸèƒ½åˆ†ææŠ¥å‘Šã€‚

## åŸºå› ä¿¡æ¯
- åŸºå› ç¬¦å·: {gene_symbol}
- åŸºå› æ•°æ®: {gene_data}

## è¯·æä¾›ä»¥ä¸‹å†…å®¹ï¼š

### 1. åŸºå› æ¦‚è¿°
ç®€æ˜æ‰¼è¦åœ°ä»‹ç»è¯¥åŸºå› çš„åŸºæœ¬ä¿¡æ¯å’Œä¸»è¦åŠŸèƒ½ã€‚

### 2. åˆ†å­åŠŸèƒ½
è¯¦ç»†è§£é‡Šè¯¥åŸºå› ç¼–ç è›‹ç™½è´¨çš„åˆ†å­åŠŸèƒ½å’Œç”Ÿç‰©å­¦ä½œç”¨ã€‚

### 3. ç”Ÿç‰©å­¦é€šè·¯
æè¿°è¯¥åŸºå› å‚ä¸çš„ä¸»è¦ç”Ÿç‰©å­¦é€šè·¯å’Œè°ƒæ§ç½‘ç»œã€‚

### 4. ç–¾ç—…å…³è”
æ€»ç»“è¯¥åŸºå› ä¸äººç±»ç–¾ç—…çš„å…³è”æ€§å’Œä¸´åºŠæ„ä¹‰ã€‚

### 5. ç ”ç©¶ä»·å€¼
è¯„ä¼°è¯¥åŸºå› åœ¨ç§‘å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨ä¸­çš„ä»·å€¼ã€‚

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€æ’°å†™ï¼Œé€‚åˆç§‘ç ”äººå‘˜å’Œä¸´åºŠåŒ»ç”Ÿé˜…è¯»ã€‚
"""
        else:
            prompt_template = """
You are a professional molecular biologist and genomics expert. Please generate a professional and accessible gene function analysis report based on the following gene information.

## Gene Information
- Gene Symbol: {gene_symbol}
- Gene Data: {gene_data}

## Please provide the following content:

### 1. Gene Overview
Provide a concise introduction to the basic information and main functions of this gene.

### 2. Molecular Function
Explain in detail the molecular function and biological role of the protein encoded by this gene.

### 3. Biological Pathways
Describe the main biological pathways and regulatory networks this gene participates in.

### 4. Disease Associations
Summarize the associations between this gene and human diseases, and its clinical significance.

### 5. Research Value
Evaluate the value of this gene in scientific research and clinical applications.

Please write in professional but accessible language, suitable for researchers and clinicians.
"""

        # å‡†å¤‡åŸºå› æ•°æ®æ‘˜è¦
        gene_data_summary = self._format_gene_data(gene_info, language)

        # æ„å»ºå®Œæ•´æç¤º
        prompt = prompt_template.format(
            gene_symbol=gene_symbol,
            gene_data=gene_data_summary
        )

        # ç”Ÿæˆè§£é‡Š
        interpretation = await self._generate_text(prompt, max_length=1536)

        if interpretation:
            # æ¸…ç†æ€è€ƒè¿‡ç¨‹
            cleaned_interpretation = self._clean_thinking_process(interpretation, show_thinking)
            return cleaned_interpretation
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šè¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè®©è°ƒç”¨æ–¹ä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•
            return ""

    async def generate_drug_interpretation(
        self,
        gene_symbol: str,
        drug_targets: List[Dict[str, Any]],
        language: str = "en",
        show_thinking: bool = False
    ) -> str:
        """
        ç”Ÿæˆè¯ç‰©é¶ç‚¹çš„æ™ºèƒ½è§£é‡Š

        Args:
            gene_symbol: åŸºå› ç¬¦å·
            drug_targets: è¯ç‰©é¶ç‚¹æ•°æ®
            language: è¯­è¨€ ("zh" æˆ– "en")
            show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹

        Returns:
            æ™ºèƒ½ç”Ÿæˆçš„è¯ç‰©é¶ç‚¹è§£é‡Š
        """

        # æ„å»ºæç¤ºæ¨¡æ¿
        if language == "zh":
            prompt_template = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¯ç‰©åŒ–å­¦å®¶å’Œä¸´åºŠè¯ç†å­¦ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹è¯ç‰©é¶ç‚¹ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šä¸”æ˜“æ‡‚çš„æ²»ç–—æ„ä¹‰åˆ†ææŠ¥å‘Šã€‚

## é¶ç‚¹ä¿¡æ¯
- é¶ç‚¹åŸºå› : {gene_symbol}
- ç›¸å…³è¯ç‰©: {drug_data}

## è¯·æä¾›ä»¥ä¸‹å†…å®¹ï¼š

### 1. é¶ç‚¹æ¦‚è¿°
ç®€æ˜æ‰¼è¦åœ°ä»‹ç»è¯¥åŸºå› ä½œä¸ºè¯ç‰©é¶ç‚¹çš„åŸºæœ¬ç‰¹å¾ã€‚

### 2. ç°æœ‰è¯ç‰©
æ€»ç»“ç›®å‰å·²çŸ¥é¶å‘è¯¥åŸºå› çš„è¯ç‰©åŠå…¶ä½œç”¨æœºåˆ¶ã€‚

### 3. æ²»ç–—åº”ç”¨
æè¿°è¿™äº›è¯ç‰©åœ¨ä¸´åºŠæ²»ç–—ä¸­çš„åº”ç”¨å’Œæ•ˆæœã€‚

### 4. å¼€å‘å‰æ™¯
è¯„ä¼°è¯¥é¶ç‚¹åœ¨æ–°è¯å¼€å‘ä¸­çš„æ½œåŠ›å’Œæœºä¼šã€‚

### 5. ä¸´åºŠæ„ä¹‰
åˆ†æè¯¥é¶ç‚¹åœ¨ç²¾å‡†åŒ»ç–—å’Œä¸ªä½“åŒ–æ²»ç–—ä¸­çš„ä»·å€¼ã€‚

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€æ’°å†™ï¼Œé€‚åˆç§‘ç ”äººå‘˜å’Œä¸´åºŠåŒ»ç”Ÿé˜…è¯»ã€‚
"""
        else:
            prompt_template = """
You are a professional medicinal chemist and clinical pharmacology expert. Please generate a professional and accessible therapeutic significance analysis report based on the following drug target information.

## Target Information
- Target Gene: {gene_symbol}
- Related Drugs: {drug_data}

## Please provide the following content:

### 1. Target Overview
Provide a concise introduction to the basic characteristics of this gene as a drug target.

### 2. Existing Drugs
Summarize currently known drugs targeting this gene and their mechanisms of action.

### 3. Therapeutic Applications
Describe the clinical applications and effects of these drugs in treatment.

### 4. Development Prospects
Evaluate the potential and opportunities of this target in new drug development.

### 5. Clinical Significance
Analyze the value of this target in precision medicine and personalized therapy.

Please write in professional but accessible language, suitable for researchers and clinicians.
"""

        # å‡†å¤‡è¯ç‰©æ•°æ®æ‘˜è¦
        drug_data_summary = self._format_drug_data(drug_targets, language)

        # æ„å»ºå®Œæ•´æç¤º
        prompt = prompt_template.format(
            gene_symbol=gene_symbol,
            drug_data=drug_data_summary
        )

        # ç”Ÿæˆè§£é‡Š
        interpretation = await self._generate_text(prompt, max_length=1536)

        if interpretation:
            # æ¸…ç†æ€è€ƒè¿‡ç¨‹
            cleaned_interpretation = self._clean_thinking_process(interpretation, show_thinking)
            return cleaned_interpretation
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šè¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œè®©è°ƒç”¨æ–¹ä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•
            return ""

    def update_config(self, new_config: Dict[str, Any]) -> tuple[bool, str]:
        """
        æ›´æ–°LLMé…ç½®

        Args:
            new_config: æ–°çš„é…ç½®å­—å…¸

        Returns:
            tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, æ¶ˆæ¯)
        """
        # éªŒè¯é…ç½®
        is_valid, message = self.config_manager.validate_config(new_config)
        if not is_valid:
            return False, message

        # ä¿å­˜é…ç½®
        if not self.config_manager.save_config(new_config):
            return False, "ä¿å­˜é…ç½®å¤±è´¥"

        # æ£€æŸ¥æ˜¯å¦æœ‰å½±å“è¿æ¥çš„å…³é”®å‚æ•°å˜æ›´
        old_provider = self.provider_name
        old_enabled = self.enabled

        new_provider = new_config.get("provider", "ollama")
        new_enabled = new_config.get("enabled", True)

        # æ£€æŸ¥å…³é”®è¿æ¥å‚æ•°æ˜¯å¦å‘ç”Ÿå˜åŒ–
        connection_params_changed = (
            old_provider != new_provider or
            old_enabled != new_enabled
        )

        # æ›´æ–°å½“å‰é…ç½®
        self.config = new_config
        self.provider_name = new_provider
        self.enabled = new_enabled

        # é‡æ–°åˆ›å»ºæä¾›å•†å®ä¾‹
        if connection_params_changed:
            self.provider = self._create_provider()
            self.is_available = False

        return True, "é…ç½®æ›´æ–°æˆåŠŸ"

    async def test_connection(self, skip_model_check: bool = False) -> tuple[bool, str, List[str]]:
        """
        æµ‹è¯•LLMè¿æ¥

        Args:
            skip_model_check: æ˜¯å¦è·³è¿‡æŒ‡å®šæ¨¡å‹æ£€æŸ¥ï¼ˆç”¨äºå¿«é€Ÿè®¾ç½®ï¼‰

        Returns:
            tuple[bool, str, List[str]]: (æ˜¯å¦æˆåŠŸ, æ¶ˆæ¯, å¯ç”¨æ¨¡å‹åˆ—è¡¨)
        """
        if not self.enabled:
            self.is_available = False
            return False, "LLMæœåŠ¡å·²ç¦ç”¨", []

        try:
            # ä½¿ç”¨æä¾›å•†æµ‹è¯•è¿æ¥
            success, message, models = await self.provider.test_connection()
            if success:
                self.is_available = True
            else:
                self.is_available = False
            return success, message, models

        except Exception as e:
            self.is_available = False
            return False, f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}", []

    def get_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰é…ç½®"""
        return self.config.copy()

    def get_status(self) -> Dict[str, Any]:
        """
        è·å–LLMæœåŠ¡çŠ¶æ€
        """
        provider_config = self.config.get(self.provider_name, {})
        return {
            "available": self.is_available,
            "enabled": self.enabled,
            "provider": self.provider_name,
            "provider_name": self.provider.get_provider_name() if self.provider else "Unknown",
            "model": provider_config.get("model_name", ""),
            "base_url": provider_config.get("base_url", ""),
            "temperature": provider_config.get("temperature", 0.3),
            "max_tokens": provider_config.get("max_tokens", 2048),
            "timeout": provider_config.get("timeout", 60)
        }
