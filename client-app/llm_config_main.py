#!/usr/bin/env python3
"""
ä¸»é¡µé¢LLMé…ç½®ç»„ä»¶
"""

import streamlit as st
import asyncio
import time
from llm_service import LLMService


def render_main_llm_config(llm_service: LLMService, language: str = "en") -> bool:
    """
    åœ¨ä¸»é¡µé¢æ¸²æŸ“è¯¦ç»†çš„LLMé…ç½®ç•Œé¢

    Args:
        llm_service: LLMæœåŠ¡å®ä¾‹
        language: ç•Œé¢è¯­è¨€

    Returns:
        bool: é…ç½®æ˜¯å¦æœ‰å˜æ›´
    """
    # åˆå§‹åŒ–è¿”å›å€¼
    config_changed = False

    # è·å–å½“å‰é…ç½®
    current_config = llm_service.get_config()
    status = llm_service.get_status()

    # åˆå§‹åŒ– session state
    if 'config_use_llm' not in st.session_state:
        st.session_state.config_use_llm = current_config.get("enabled", True)
    if 'config_provider' not in st.session_state:
        st.session_state.config_provider = current_config.get("provider", "ollama")

    # ç•Œé¢æ–‡æœ¬
    texts = {
        "zh": {
            "llm_config_title": "LLM é…ç½®",
            "enable_llm": "å¯ç”¨ LLM å¢å¼ºåˆ†æ",
            "use_basic_analysis": "ä½¿ç”¨åŸºç¡€åˆ†æ",
            "provider": "LLM æä¾›å•†",
            "server_url": "æœåŠ¡åœ°å€",
            "model_name": "æ¨¡å‹",
            "api_key": "API å¯†é’¥",
            "test_connection": "æµ‹è¯•è¿æ¥",
            "save_config": "ä¿å­˜é…ç½®",
            "close_config": "å…³é—­é…ç½®",
            "testing": "æµ‹è¯•ä¸­...",
            "test_success": "âœ… è¿æ¥æˆåŠŸ",
            "test_failed": "âŒ è¿æ¥å¤±è´¥",
            "available_models": "å¯ç”¨æ¨¡å‹:",
            "config_saved": "âœ… é…ç½®ä¿å­˜æˆåŠŸ",
            "config_save_failed": "âŒ é…ç½®ä¿å­˜å¤±è´¥",
            "ollama_desc": "æœ¬åœ° Ollama æœåŠ¡",
            "deepseek_desc": "DeepSeek å®˜æ–¹ API",
            "gemini_desc": "Google Gemini API",
            "api_key_placeholder": "è¯·è¾“å…¥ DeepSeek API å¯†é’¥",
            "api_key_help": "ä» https://platform.deepseek.com/api_keys è·å–",
            "gemini_api_key_placeholder": "è¯·è¾“å…¥ Gemini API å¯†é’¥",
            "gemini_api_key_help": "ä» https://aistudio.google.com/app/apikey è·å–",
            "basic_analysis_warning": "âš ï¸ é€‰æ‹©åŸºç¡€åˆ†æå°†æ¸…ç©ºå½“å‰ LLM é…ç½®",
            "confirm_basic_analysis": "ç¡®è®¤ä½¿ç”¨åŸºç¡€åˆ†æ",
            "llm_config_cleared": "âœ… LLM é…ç½®å·²æ¸…ç©ºï¼Œå°†ä½¿ç”¨åŸºç¡€åˆ†æ"
        },
        "en": {
            "llm_config_title": "LLM Configuration",
            "enable_llm": "Enable LLM Analysis",
            "use_basic_analysis": "Use Basic Analysis",
            "provider": "LLM Provider",
            "server_url": "Server URL",
            "model_name": "Model",
            "api_key": "API Key",
            "test_connection": "Test Connection",
            "save_config": "Save Config",
            "close_config": "Close Config",
            "testing": "Testing...",
            "test_success": "âœ… Connected",
            "test_failed": "âŒ Failed",
            "available_models": "Available models:",
            "config_saved": "âœ… Configuration saved",
            "config_save_failed": "âŒ Failed to save",
            "ollama_desc": "Local Ollama Service",
            "deepseek_desc": "DeepSeek Official API",
            "gemini_desc": "Google Gemini API",
            "api_key_placeholder": "Enter DeepSeek API Key",
            "api_key_help": "Get from https://platform.deepseek.com/api_keys",
            "gemini_api_key_placeholder": "Enter Gemini API Key",
            "gemini_api_key_help": "Get from https://aistudio.google.com/app/apikey",
            "basic_analysis_warning": "âš ï¸ Choosing basic analysis will clear current LLM configuration",
            "confirm_basic_analysis": "Confirm Basic Analysis",
            "llm_config_cleared": "âœ… LLM configuration cleared, using basic analysis"
        }
    }

    t = texts.get(language, texts["en"])

    # é…ç½®ç•Œé¢æ ‡é¢˜å’Œå…³é—­æŒ‰é’®
    col1, col2 = st.columns([4, 1])
    with col1:
        st.header(t["llm_config_title"])
    with col2:
        if st.button("âœ–ï¸ Close", key="close_config_header", help="Close LLM configuration"):
            st.session_state.show_main_llm_config = False
            st.rerun()

    # åˆ†ææ¨¡å¼é€‰æ‹©ï¼ˆåœ¨è¡¨å•å¤–ï¼Œå¯ä»¥ç«‹å³å“åº”ï¼‰
    col1, col2 = st.columns(2)

    with col1:
        # ä½¿ç”¨ç‹¬ç«‹çš„ key æ¥é¿å…çŠ¶æ€å†²çª
        analysis_key = "llm_analysis_mode_selector"

        # åˆå§‹åŒ–ç‹¬ç«‹é”®å€¼ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æ—¶è®¾ç½®ï¼‰
        if analysis_key not in st.session_state:
            st.session_state[analysis_key] = st.session_state.config_use_llm

        use_llm = st.radio(
            "Analysis Mode",
            options=[True, False],
            format_func=lambda x: t["enable_llm"] if x else t["use_basic_analysis"],
            key=analysis_key
        )

        # æ›´æ–°é…ç½®çŠ¶æ€
        st.session_state.config_use_llm = use_llm

    with col2:
        if not use_llm:
            st.warning(t["basic_analysis_warning"])
            if st.button(t["confirm_basic_analysis"], type="primary", use_container_width=True):
                # ç¡®è®¤é€‰æ‹©åŸºç¡€åˆ†æ
                config = _build_config("ollama", False, "", "", "", 0.3, 2048, 60, current_config)
                success, message = llm_service.update_config(config)

                if success:
                    # æ ‡è®°ç”¨æˆ·ä¸»åŠ¨é€‰æ‹©åŸºç¡€åˆ†æ
                    st.session_state.user_chose_basic_analysis = True
                    st.session_state.llm_configured_via_sidebar = False
                    st.session_state.llm_setup_completed = True
                    st.session_state.show_main_llm_config = False

                    st.success(t["llm_config_cleared"])
                    time.sleep(1.5)
                    st.rerun()
                else:
                    st.error(f"{t['config_save_failed']} {message}")

    # LLM æä¾›å•†é€‰æ‹©ï¼ˆåœ¨è¡¨å•å¤–ï¼‰
    provider = "ollama"  # é»˜è®¤å€¼
    if use_llm:
        provider_options = {
            "ollama": t["ollama_desc"],
            "deepseek": t["deepseek_desc"],
            "gemini": t["gemini_desc"]
        }

        # ä½¿ç”¨ç‹¬ç«‹çš„ key æ¥é¿å…çŠ¶æ€å†²çª
        provider_key = "llm_provider_selector"

        # åˆå§‹åŒ–ç‹¬ç«‹é”®å€¼ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æ—¶è®¾ç½®ï¼‰
        if provider_key not in st.session_state:
            st.session_state[provider_key] = st.session_state.config_provider

        provider = st.selectbox(
            t["provider"],
            options=list(provider_options.keys()),
            format_func=lambda x: provider_options[x],
            key=provider_key
        )

        # æ›´æ–°é…ç½®çŠ¶æ€
        st.session_state.config_provider = provider

    # Ollama æ¨¡å‹è·å–ï¼ˆåœ¨è¡¨å•å¤–ï¼‰
    if use_llm and provider == "ollama":
        # åˆå§‹åŒ– session state å­˜å‚¨æ¨¡å‹åˆ—è¡¨
        if 'ollama_models' not in st.session_state:
            st.session_state.ollama_models = []
        if 'ollama_models_error' not in st.session_state:
            st.session_state.ollama_models_error = None
        if 'ollama_base_url' not in st.session_state:
            st.session_state.ollama_base_url = current_config.get("ollama", {}).get("base_url", "http://localhost:11434")

        # æœåŠ¡åœ°å€è¾“å…¥ï¼ˆåœ¨è¡¨å•å¤–ï¼Œå¯ä»¥ç«‹å³å“åº”ï¼‰
        st.subheader("ğŸ”§ Ollama Service Configuration")

        base_url = st.text_input(
            t["server_url"],
            value=st.session_state.ollama_base_url,
            placeholder="http://localhost:11434",
            help="Please enter the correct Ollama service address first",
            key="ollama_base_url_input"
        )

        # æ›´æ–° session state
        st.session_state.ollama_base_url = base_url

        # è·å–æ¨¡å‹åˆ—è¡¨æŒ‰é’®
        col1, col2 = st.columns([1, 3])
        with col1:
            get_models_clicked = st.button("ğŸ“‹ Get Model List", help="Get available models from Ollama service")

        with col2:
            if st.session_state.ollama_models:
                st.success(f"âœ… Retrieved {len(st.session_state.ollama_models)} models")
            elif st.session_state.ollama_models_error:
                st.error(f"âŒ {st.session_state.ollama_models_error}")

        # å¤„ç†è·å–æ¨¡å‹åˆ—è¡¨
        if get_models_clicked:
            with st.spinner("Getting model list..."):
                try:
                    import requests
                    response = requests.get(f"{base_url}/api/tags", timeout=10)
                    if response.status_code == 200:
                        data = response.json()
                        models = [model['name'] for model in data.get('models', [])]
                        if models:
                            st.session_state.ollama_models = models
                            st.session_state.ollama_models_error = None
                            st.success(f"âœ… Successfully retrieved {len(models)} models")
                        else:
                            st.session_state.ollama_models = []
                            st.session_state.ollama_models_error = "No available models found"
                            st.warning("âš ï¸ No available models found, please ensure models are installed in Ollama")
                    else:
                        st.session_state.ollama_models = []
                        st.session_state.ollama_models_error = f"HTTP {response.status_code}"
                        st.error(f"âŒ Failed to get model list: HTTP {response.status_code}")
                except Exception as e:
                    st.session_state.ollama_models = []
                    st.session_state.ollama_models_error = str(e)
                    st.error(f"âŒ Failed to connect to Ollama service: {e}")

                # å¼ºåˆ¶é‡æ–°è¿è¡Œä»¥æ›´æ–°çŠ¶æ€æ˜¾ç¤º
                time.sleep(0.5)
                st.rerun()

    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ˜¾ç¤ºé…ç½®è¡¨å•
    show_ollama_form = False
    if use_llm and provider == "ollama":
        # åˆå§‹åŒ–æ‰‹åŠ¨é…ç½®çŠ¶æ€
        if 'ollama_manual_config' not in st.session_state:
            st.session_state.ollama_manual_config = False

        # åªæœ‰åœ¨è·å–åˆ°æ¨¡å‹åˆ—è¡¨æˆ–ç”¨æˆ·æ˜ç¡®è¦æ±‚æ‰‹åŠ¨é…ç½®æ—¶æ‰æ˜¾ç¤ºè¡¨å•
        if st.session_state.ollama_models:
            show_ollama_form = True
        elif st.session_state.ollama_models_error:
            # è·å–å¤±è´¥ï¼Œæ˜¾ç¤ºæ‰‹åŠ¨é…ç½®é€‰é¡¹
            st.warning("âš ï¸ Unable to automatically get model list, you can manually enter model name")
            col1, col2 = st.columns([1, 3])
            with col1:
                if st.button("ğŸ”§ Manual Model Config", help="If unable to get model list, you can manually enter configuration"):
                    st.session_state.ollama_manual_config = True

            if st.session_state.ollama_manual_config:
                show_ollama_form = True
        else:
            # è¿˜æ²¡æœ‰å°è¯•è·å–æ¨¡å‹åˆ—è¡¨
            st.info("ğŸ’¡ Please click the 'ğŸ“‹ Get Model List' button above to get available models")

    # è¯¦ç»†é…ç½®è¡¨å•
    if show_ollama_form or (use_llm and provider in ["deepseek", "gemini"]):
        with st.form("main_llm_config_form", clear_on_submit=False):
            # æ ¹æ®é€‰æ‹©çš„æä¾›å•†å’Œæ¨¡å¼æ˜¾ç¤ºç›¸åº”é…ç½®
            if use_llm and provider == "ollama" and show_ollama_form:
                ollama_config = current_config.get("ollama", {})

                # ä½¿ç”¨ session state ä¸­çš„åœ°å€
                base_url = st.session_state.ollama_base_url

                # æ˜¾ç¤ºæ¨¡å‹é€‰æ‹©
                if st.session_state.ollama_models:
                    # æœ‰å¯ç”¨æ¨¡å‹ï¼Œæ˜¾ç¤ºé€‰æ‹©æ¡†
                    current_model = ollama_config.get("model_name", "")
                    if current_model not in st.session_state.ollama_models and st.session_state.ollama_models:
                        current_model = st.session_state.ollama_models[0]

                    model_name = st.selectbox(
                        t["model_name"],
                        options=st.session_state.ollama_models,
                        index=st.session_state.ollama_models.index(current_model) if current_model in st.session_state.ollama_models else 0,
                        help="Select an available model from Ollama service"
                    )
                else:
                    # æ‰‹åŠ¨é…ç½®æ¨¡å¼ï¼Œæ˜¾ç¤ºæ–‡æœ¬è¾“å…¥æ¡†
                    st.info("ğŸ”§ Manual configuration mode: Please ensure you have installed the corresponding model in Ollama")
                    model_name = st.text_input(
                        t["model_name"],
                        value=ollama_config.get("model_name", ""),
                        placeholder="e.g.: deepseek-r1:1.5b, llama2:7b, qwen:7b",
                        help="Please enter the model name installed in Ollama, you can use 'ollama list' command to view installed models"
                    )

                    # æä¾›ä¸€äº›å¸¸ç”¨æ¨¡å‹çš„å»ºè®®
                    st.markdown("**Common Model Suggestions:**")
                    st.markdown("- `deepseek-r1:1.5b` - DeepSeek R1 1.5B model")
                    st.markdown("- `llama2:7b` - Llama 2 7B model")
                    st.markdown("- `qwen:7b` - Qwen 7B model")
                    st.markdown("- `mistral:7b` - Mistral 7B model")

                temperature = ollama_config.get("temperature", 0.3)
                max_tokens = ollama_config.get("max_tokens", 2048)
                timeout = ollama_config.get("timeout", 60)
                api_key = ""

            elif use_llm and provider == "deepseek":
                deepseek_config = current_config.get("deepseek", {})

                api_key = st.text_input(
                    t["api_key"],
                    value=deepseek_config.get("api_key", ""),
                    placeholder=t["api_key_placeholder"],
                    type="password",
                    help=t["api_key_help"]
                )

                base_url = deepseek_config.get("base_url", "https://api.deepseek.com")
                model_name = "deepseek-chat"  # å›ºå®šä½¿ç”¨ deepseek-chat

                temperature = deepseek_config.get("temperature", 0.3)
                max_tokens = deepseek_config.get("max_tokens", 2048)
                timeout = deepseek_config.get("timeout", 60)

            elif use_llm and provider == "gemini":
                gemini_config = current_config.get("gemini", {})

                api_key = st.text_input(
                    t["api_key"],
                    value=gemini_config.get("api_key", ""),
                    placeholder=t["gemini_api_key_placeholder"],
                    type="password",
                    help=t["gemini_api_key_help"]
                )

                base_url = gemini_config.get("base_url", "https://generativelanguage.googleapis.com")

                # Geminiæ¨¡å‹é€‰æ‹©
                gemini_models = [
                    "gemini-1.5-flash",
                    "gemini-1.5-pro",
                    "gemini-1.0-pro"
                ]

                current_model = gemini_config.get("model_name", "gemini-1.5-flash")
                model_name = st.selectbox(
                    t["model_name"],
                    options=gemini_models,
                    index=gemini_models.index(current_model) if current_model in gemini_models else 0
                )

                temperature = gemini_config.get("temperature", 0.3)
                max_tokens = gemini_config.get("max_tokens", 2048)
                timeout = gemini_config.get("timeout", 60)
            else:
                # ä½¿ç”¨åŸºç¡€åˆ†ææ—¶çš„é»˜è®¤å€¼
                base_url = ""
                model_name = ""
                api_key = ""
                temperature = 0.3
                max_tokens = 2048
                timeout = 60

            # æŒ‰é’®å¸ƒå±€
            col1, col2, col3 = st.columns(3)

            with col1:
                test_clicked = st.form_submit_button(
                    t["test_connection"],
                    disabled=not use_llm,
                    use_container_width=True
                )

            with col2:
                save_clicked = st.form_submit_button(
                    t["save_config"],
                    type="primary",
                    use_container_width=True
                )

            with col3:
                close_clicked = st.form_submit_button(
                    t["close_config"],
                    use_container_width=True
                )
    else:
        # æ²¡æœ‰è¡¨å•æ—¶åˆå§‹åŒ–æŒ‰é’®å˜é‡
        test_clicked = False
        save_clicked = False

        # æ²¡æœ‰è¡¨å•æ—¶ï¼Œåœ¨é¡µé¢åº•éƒ¨æ˜¾ç¤ºå…³é—­æŒ‰é’®
        st.markdown("---")
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            close_clicked = st.button(
                t["close_config"],
                type="secondary",
                use_container_width=True,
                key="close_config_no_form"
            )

        # åˆå§‹åŒ–é…ç½®å˜é‡
        base_url = ""
        model_name = ""
        api_key = ""
        temperature = 0.3
        max_tokens = 2048
        timeout = 60

    # å¤„ç†æµ‹è¯•æŒ‰é’®
    if test_clicked and use_llm:
        with st.spinner(t["testing"]):
            test_config = _build_config(provider, use_llm, base_url, model_name, api_key, temperature, max_tokens, timeout, current_config)
            temp_service = LLMService(test_config)
            success, message, models = asyncio.run(temp_service.test_connection())
            
            if success:
                st.success(t["test_success"])
                if models:
                    st.info(f"{t['available_models']} {', '.join(models)}")
            else:
                st.error(f"{t['test_failed']} {message}")
    
    # å¤„ç†ä¿å­˜æŒ‰é’®
    if save_clicked:
        new_config = _build_config(provider, use_llm, base_url, model_name, api_key, temperature, max_tokens, timeout, current_config)
        success, message = llm_service.update_config(new_config)

        if success:
            if not use_llm:
                # å¦‚æœé€‰æ‹©åŸºç¡€åˆ†æï¼Œæ¸…ç©º LLM é…ç½®å¹¶é‡ç½®è®¾ç½®çŠ¶æ€
                st.session_state.llm_setup_completed = False
                st.session_state.user_chose_basic_analysis = True  # æ ‡è®°ç”¨æˆ·ä¸»åŠ¨é€‰æ‹©åŸºç¡€åˆ†æ
                st.session_state.llm_configured_via_sidebar = False
                st.success(t["llm_config_cleared"])
            else:
                st.success(t["config_saved"])
                # æ ‡è®°ç”¨æˆ·é€šè¿‡ä¾§è¾¹æ é…ç½®äº† LLM
                st.session_state.llm_configured_via_sidebar = True
                st.session_state.llm_setup_completed = True
                st.session_state.user_chose_basic_analysis = False

                # ç«‹å³é‡æ–°åˆå§‹åŒ– LLM æœåŠ¡ä»¥æ›´æ–°çŠ¶æ€
                with st.spinner("Applying new configuration..."):
                    init_success = asyncio.run(llm_service.initialize())
                    if init_success:
                        st.success("âœ… LLM service successfully reinitialized")
                    else:
                        st.warning("âš ï¸ LLM service reinitialization failed, but configuration has been saved")

            config_changed = True
            st.session_state.show_main_llm_config = False
            time.sleep(1.5)
            st.rerun()
        else:
            st.error(f"{t['config_save_failed']} {message}")

    # å¤„ç†å…³é—­æŒ‰é’®
    if close_clicked:
        st.session_state.show_main_llm_config = False
        st.rerun()

    return config_changed


def _build_config(provider, enabled, base_url, model_name, api_key, temperature, max_tokens, timeout, current_config):
    """æ„å»ºé…ç½®å­—å…¸"""
    if not enabled:
        # å¦‚æœç¦ç”¨ LLMï¼Œè¿”å›ä¸€ä¸ªæ¸…ç©ºçš„é…ç½®
        return {
            "provider": "ollama",
            "enabled": False,
            "ollama": {
                "base_url": "http://localhost:11434",
                "model_name": "deepseek-r1:1.5b",
                "temperature": 0.3,
                "max_tokens": 2048,
                "timeout": 60
            },
            "deepseek": {
                "api_key": "",
                "base_url": "https://api.deepseek.com",
                "model_name": "deepseek-chat",
                "temperature": 0.3,
                "max_tokens": 2048,
                "timeout": 60
            },
            "gemini": {
                "api_key": "",
                "base_url": "https://generativelanguage.googleapis.com",
                "model_name": "gemini-1.5-flash",
                "temperature": 0.3,
                "max_tokens": 2048,
                "timeout": 60
            }
        }
    
    new_config = {
        "provider": provider,
        "enabled": enabled
    }
    
    if provider == "ollama":
        new_config["ollama"] = {
            "base_url": base_url,
            "model_name": model_name,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": timeout
        }
        new_config["deepseek"] = current_config.get("deepseek", {})
    elif provider == "deepseek":
        new_config["deepseek"] = {
            "api_key": api_key,
            "base_url": base_url,
            "model_name": model_name,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": timeout
        }
        new_config["ollama"] = current_config.get("ollama", {})
        new_config["gemini"] = current_config.get("gemini", {})
    elif provider == "gemini":
        new_config["gemini"] = {
            "api_key": api_key,
            "base_url": base_url,
            "model_name": model_name,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": timeout
        }
        new_config["ollama"] = current_config.get("ollama", {})
        new_config["deepseek"] = current_config.get("deepseek", {})

    return new_config
