# client-app/llm_first_setup.py
"""
LLMé¦–æ¬¡è®¾ç½®ç•Œé¢
"""

import streamlit as st
import asyncio
from typing import Dict, Any, Optional
from llm_service import LLMService

def render_llm_first_time_setup(language: str = "en") -> Optional[Dict[str, Any]]:
    """
    æ¸²æŸ“LLMé¦–æ¬¡è®¾ç½®ç•Œé¢ï¼Œæ”¯æŒé€‰æ‹©æä¾›å•†
    
    Args:
        language: ç•Œé¢è¯­è¨€
        
    Returns:
        Dict[str, Any]: å¦‚æœç”¨æˆ·å®Œæˆè®¾ç½®ï¼Œè¿”å›é…ç½®å­—å…¸ï¼Œå¦åˆ™è¿”å›None
    """
    texts = {
        "zh": {
            "title": "ğŸ¤– LLM æ™ºèƒ½åˆ†æè®¾ç½®",
            "welcome": "æ¬¢è¿ä½¿ç”¨å› æœåˆ†æç³»ç»Ÿï¼",
            "description": "ä¸ºäº†æä¾›æ›´æ™ºèƒ½çš„åˆ†æè§£é‡Šï¼Œè¯·é€‰æ‹©å¹¶é…ç½®æ‚¨çš„ LLM æœåŠ¡ï¼š",
            "provider_selection": "é€‰æ‹© LLM æä¾›å•†",
            "ollama_option": "ğŸ  Ollama (æœ¬åœ°éƒ¨ç½²)",
            "deepseek_option": "ğŸŒ DeepSeek API (å®˜æ–¹æœåŠ¡)",
            "gemini_option": "ğŸ§  Google Gemini API",
            "skip_option": "â­ï¸ è·³è¿‡è®¾ç½® (ä½¿ç”¨åŸºç¡€åˆ†æ)",
            "ollama_desc": "â€¢ å®Œå…¨æœ¬åœ°è¿è¡Œï¼Œä¿æŠ¤æ•°æ®éšç§\nâ€¢ éœ€è¦æœ¬åœ°å®‰è£…å’Œé…ç½®\nâ€¢ é€‚åˆæœ‰æŠ€æœ¯èƒŒæ™¯çš„ç”¨æˆ·",
            "deepseek_desc": "â€¢ å®˜æ–¹ API æœåŠ¡ï¼Œå³å¼€å³ç”¨\nâ€¢ éœ€è¦ API å¯†é’¥ï¼ŒæŒ‰ä½¿ç”¨é‡ä»˜è´¹\nâ€¢ å“åº”é€Ÿåº¦å¿«ï¼Œåˆ†æè´¨é‡é«˜",
            "gemini_desc": "â€¢ Google æœ€æ–° AI æŠ€æœ¯\nâ€¢ æ”¯æŒå¤šç§æ¨¡å‹é€‰æ‹©\nâ€¢ å¼ºå¤§çš„åˆ†æå’Œç†è§£èƒ½åŠ›",
            "skip_desc": "â€¢ ä½¿ç”¨åŸºç¡€çš„è§„åˆ™åˆ†æ\nâ€¢ åç»­å¯åœ¨è®¾ç½®ä¸­é…ç½® LLM",
            "continue_btn": "ç»§ç»­é…ç½®",
            "skip_btn": "è·³è¿‡è®¾ç½®",
            "ollama_setup_title": "é…ç½® Ollama æœ¬åœ°æœåŠ¡",
            "deepseek_setup_title": "é…ç½® DeepSeek API",
            "gemini_setup_title": "é…ç½® Google Gemini API",
            "server_url": "æœåŠ¡åœ°å€",
            "api_key": "API å¯†é’¥",
            "api_key_help": "ä» https://platform.deepseek.com/api_keys è·å–",
            "gemini_api_key_help": "ä» https://aistudio.google.com/app/apikey è·å–",
            "model_selection": "é€‰æ‹©æ¨¡å‹",
            "test_connection": "æµ‹è¯•è¿æ¥",
            "complete_setup": "å®Œæˆè®¾ç½®",
            "back": "è¿”å›",
            "testing": "æ­£åœ¨æµ‹è¯•è¿æ¥...",
            "test_success": "âœ… è¿æ¥æˆåŠŸï¼",
            "test_failed": "âŒ è¿æ¥å¤±è´¥ï¼š",
            "setup_complete": "ğŸ‰ LLM è®¾ç½®å®Œæˆï¼",
            "getting_models": "æ­£åœ¨è·å–æ¨¡å‹åˆ—è¡¨...",
            "no_models": "æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹"
        },
        "en": {
            "title": "ğŸ¤– LLM Intelligent Analysis Setup",
            "welcome": "Welcome to the Causal Analysis System!",
            "description": "To provide smarter analysis explanations, please select and configure your LLM service:",
            "provider_selection": "Choose LLM Provider",
            "ollama_option": "ğŸ  Ollama (Local Deployment)",
            "deepseek_option": "ğŸŒ DeepSeek API (Official Service)",
            "gemini_option": "ğŸ§  Google Gemini API",
            "skip_option": "â­ï¸ Skip Setup (Basic Analysis)",
            "ollama_desc": "â€¢ Runs completely locally, protects data privacy\nâ€¢ Requires local installation and configuration\nâ€¢ Suitable for technical users",
            "deepseek_desc": "â€¢ Official API service, ready to use\nâ€¢ Requires API key, pay-per-use\nâ€¢ Fast response, high analysis quality",
            "gemini_desc": "â€¢ Google's latest AI technology\nâ€¢ Multiple model options available\nâ€¢ Powerful analysis and understanding capabilities",
            "skip_desc": "â€¢ Use basic rule-based analysis\nâ€¢ Can configure LLM later in settings",
            "continue_btn": "Continue Setup",
            "skip_btn": "Skip Setup",
            "ollama_setup_title": "Configure Ollama Local Service",
            "deepseek_setup_title": "Configure DeepSeek API",
            "gemini_setup_title": "Configure Google Gemini API",
            "server_url": "Service URL",
            "api_key": "API Key",
            "api_key_help": "Get from https://platform.deepseek.com/api_keys",
            "gemini_api_key_help": "Get from https://aistudio.google.com/app/apikey",
            "model_selection": "Select Model",
            "test_connection": "Test Connection",
            "complete_setup": "Complete Setup",
            "back": "Back",
            "testing": "Testing connection...",
            "test_success": "âœ… Connection successful!",
            "test_failed": "âŒ Connection failed:",
            "setup_complete": "ğŸ‰ LLM setup completed!",
            "getting_models": "Getting model list...",
            "no_models": "No available models found"
        }
    }
    
    t = texts.get(language, texts["zh"])
    
    # ä½¿ç”¨session stateæ¥ç®¡ç†è®¾ç½®æµç¨‹çŠ¶æ€
    if 'first_setup_step' not in st.session_state:
        st.session_state.first_setup_step = 'provider_selection'
    if 'first_setup_provider' not in st.session_state:
        st.session_state.first_setup_provider = None
    
    # åˆ›å»ºä¸€ä¸ªå®¹å™¨æ¥æ˜¾ç¤ºè®¾ç½®ç•Œé¢
    with st.container():
        st.markdown(f"### {t['title']}")
        st.markdown(f"**{t['welcome']}**")
        st.write(t["description"])
        
        # æ­¥éª¤1ï¼šé€‰æ‹©æä¾›å•†
        if st.session_state.first_setup_step == 'provider_selection':
            st.markdown(f"#### {t['provider_selection']}")
            
            # åˆ›å»ºå››åˆ—å¸ƒå±€
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.markdown(f"**{t['ollama_option']}**")
                st.markdown(t['ollama_desc'])
                if st.button("Choose Ollama",
                           key="choose_ollama", use_container_width=True):
                    st.session_state.first_setup_provider = 'ollama'
                    st.session_state.first_setup_step = 'configure'
                    st.rerun()

            with col2:
                st.markdown(f"**{t['deepseek_option']}**")
                st.markdown(t['deepseek_desc'])
                if st.button("Choose DeepSeek",
                           key="choose_deepseek", use_container_width=True):
                    st.session_state.first_setup_provider = 'deepseek'
                    st.session_state.first_setup_step = 'configure'
                    st.rerun()

            with col3:
                st.markdown(f"**{t['gemini_option']}**")
                st.markdown(t['gemini_desc'])
                if st.button("Choose Gemini",
                           key="choose_gemini", use_container_width=True):
                    st.session_state.first_setup_provider = 'gemini'
                    st.session_state.first_setup_step = 'configure'
                    st.rerun()

            with col4:
                st.markdown(f"**{t['skip_option']}**")
                st.markdown(t['skip_desc'])
                if st.button(t['skip_btn'], key="skip_setup", use_container_width=True):
                    # è¿”å›ç¦ç”¨LLMçš„é…ç½®
                    _cleanup_first_setup_state()
                    return {
                        "provider": "ollama",
                        "enabled": False,
                        "ollama": {
                            "base_url": "http://localhost:11434",
                            "model_name": "deepseek-r1:1.5b",
                            "temperature": 0.3,
                            "max_tokens": 2048,
                            "timeout": 60
                        },
                        "deepseek": {
                            "api_key": "",
                            "base_url": "https://api.deepseek.com",
                            "model_name": "deepseek-chat",
                            "temperature": 0.3,
                            "max_tokens": 2048,
                            "timeout": 60
                        },
                        "gemini": {
                            "api_key": "",
                            "base_url": "https://generativelanguage.googleapis.com",
                            "model_name": "gemini-1.5-flash",
                            "temperature": 0.3,
                            "max_tokens": 2048,
                            "timeout": 60
                        }
                    }
        
        # æ­¥éª¤2ï¼šé…ç½®é€‰å®šçš„æä¾›å•†
        elif st.session_state.first_setup_step == 'configure':
            provider = st.session_state.first_setup_provider
            
            if provider == 'ollama':
                return _render_ollama_setup(t, language)
            elif provider == 'deepseek':
                return _render_deepseek_setup(t, language)
            elif provider == 'gemini':
                return _render_gemini_setup(t, language)
    
    return None

def _render_ollama_setup(t: dict, language: str) -> Optional[Dict[str, Any]]:
    """æ¸²æŸ“ Ollama é…ç½®ç•Œé¢"""
    st.markdown(f"#### {t['ollama_setup_title']}")

    # åˆå§‹åŒ– session state
    if 'first_setup_ollama_base_url' not in st.session_state:
        st.session_state.first_setup_ollama_base_url = "http://localhost:11434"
    if 'first_setup_ollama_models' not in st.session_state:
        st.session_state.first_setup_ollama_models = []
    if 'first_setup_ollama_models_error' not in st.session_state:
        st.session_state.first_setup_ollama_models_error = None
    if 'first_setup_ollama_manual_config' not in st.session_state:
        st.session_state.first_setup_ollama_manual_config = False

    # Ollama æœåŠ¡é…ç½®
    st.markdown("##### Ollama Service Configuration")

    # æœåŠ¡åœ°å€è¾“å…¥æ¡†ï¼ˆç‹¬å ä¸€è¡Œï¼‰
    base_url = st.text_input(
        t["server_url"],
        value=st.session_state.first_setup_ollama_base_url,
        placeholder="http://localhost:11434",
        help="Please enter the correct Ollama service address first",
        key="first_setup_ollama_url_input"
    )
    # æ›´æ–° session state
    st.session_state.first_setup_ollama_base_url = base_url

    # è·å–æ¨¡å‹åˆ—è¡¨æŒ‰é’®ï¼ˆä¸å·¦ä¾§è®¾ç½®ä¿æŒä¸€è‡´çš„å¸ƒå±€ï¼‰
    col1, col2 = st.columns([1, 3])
    with col1:
        get_models_clicked = st.button("ğŸ“‹ Get Model List", help="Get available model list from Ollama service", key="first_setup_get_models")

    with col2:
        if st.session_state.first_setup_ollama_models:
            st.success(f"âœ… Retrieved {len(st.session_state.first_setup_ollama_models)} models")
        elif st.session_state.first_setup_ollama_models_error:
            st.error(f"âŒ {st.session_state.first_setup_ollama_models_error}")

    # å¤„ç†è·å–æ¨¡å‹åˆ—è¡¨
    if get_models_clicked:
        with st.spinner("Getting model list..."):
            try:
                import requests
                import time
                response = requests.get(f"{base_url}/api/tags", timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    models = [model['name'] for model in data.get('models', [])]
                    if models:
                        st.session_state.first_setup_ollama_models = models
                        st.session_state.first_setup_ollama_models_error = None
                    else:
                        st.session_state.first_setup_ollama_models = []
                        st.session_state.first_setup_ollama_models_error = "No available models found"
                else:
                    st.session_state.first_setup_ollama_models = []
                    st.session_state.first_setup_ollama_models_error = f"HTTP {response.status_code}"
            except Exception as e:
                st.session_state.first_setup_ollama_models = []
                st.session_state.first_setup_ollama_models_error = str(e)

            # å¼ºåˆ¶é‡æ–°è¿è¡Œä»¥æ›´æ–°çŠ¶æ€æ˜¾ç¤º
            time.sleep(0.5)
            st.rerun()

    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ˜¾ç¤ºé…ç½®è¡¨å•
    show_ollama_form = False
    if st.session_state.first_setup_ollama_models:
        show_ollama_form = True
    elif st.session_state.first_setup_ollama_models_error:
        # è·å–å¤±è´¥ï¼Œæ˜¾ç¤ºæ‰‹åŠ¨é…ç½®é€‰é¡¹
        st.warning("âš ï¸ Unable to automatically get model list, you can manually enter model name")
        col1, col2 = st.columns([1, 3])
        with col1:
            if st.button("ğŸ”§ Manual Model Config", help="If unable to get model list, you can manually enter configuration", key="first_setup_manual_config"):
                st.session_state.first_setup_ollama_manual_config = True

        if st.session_state.first_setup_ollama_manual_config:
            show_ollama_form = True
    else:
        # è¿˜æ²¡æœ‰å°è¯•è·å–æ¨¡å‹åˆ—è¡¨
        st.info("ğŸ’¡ Please click the 'ğŸ“‹ Get Model List' button above to get available models")

    # åˆå§‹åŒ–Ollamaæµ‹è¯•çŠ¶æ€
    if 'first_setup_ollama_test_success' not in st.session_state:
        st.session_state.first_setup_ollama_test_success = False
    if 'first_setup_ollama_test_config' not in st.session_state:
        st.session_state.first_setup_ollama_test_config = None

    # è¯¦ç»†é…ç½®è¡¨å•
    if show_ollama_form:
        with st.form("ollama_setup_form"):
            # æ˜¾ç¤ºæ¨¡å‹é€‰æ‹©
            if st.session_state.first_setup_ollama_models:
                # æœ‰å¯ç”¨æ¨¡å‹ï¼Œæ˜¾ç¤ºé€‰æ‹©æ¡†
                selected_model = st.selectbox(
                    t["model_selection"],
                    options=st.session_state.first_setup_ollama_models,
                    index=0,
                    help="Select an available model from Ollama service"
                )
            else:
                # æ‰‹åŠ¨é…ç½®æ¨¡å¼ï¼Œæ˜¾ç¤ºæ–‡æœ¬è¾“å…¥æ¡†
                st.info("ğŸ”§ Manual configuration mode: Please ensure you have installed the corresponding model in Ollama")
                selected_model = st.text_input(
                    t["model_selection"],
                    value="",
                    placeholder="e.g.: deepseek-r1:1.5b, llama2:7b, qwen:7b",
                    help="Please enter the model name installed in Ollama, you can use 'ollama list' command to view installed models"
                )

                # æä¾›ä¸€äº›å¸¸ç”¨æ¨¡å‹çš„å»ºè®®
                st.markdown("**Common Model Suggestions:**")
                st.markdown("- `deepseek-r1:1.5b` - DeepSeek R1 1.5B model")
                st.markdown("- `llama2:7b` - Llama 2 7B model")
                st.markdown("- `qwen:7b` - Qwen 7B model")
                st.markdown("- `mistral:7b` - Mistral 7B model")

            col1, col2 = st.columns(2)
            with col1:
                back_clicked = st.form_submit_button(t["back"])
            with col2:
                test_clicked = st.form_submit_button(t["test_connection"], type="primary")

            if back_clicked:
                st.session_state.first_setup_step = 'provider_selection'
                # æ¸…ç†æµ‹è¯•çŠ¶æ€
                st.session_state.first_setup_ollama_test_success = False
                st.session_state.first_setup_ollama_test_config = None
                st.rerun()

            if test_clicked:
                if not selected_model:
                    st.error("Please select or enter model name")
                else:
                    # æµ‹è¯•é…ç½®
                    test_config = {
                        "provider": "ollama",
                        "enabled": True,
                        "ollama": {
                            "base_url": base_url,
                            "model_name": selected_model,
                            "temperature": 0.3,
                            "max_tokens": 2048,
                            "timeout": 60
                        },
                        "deepseek": {
                            "api_key": "",
                            "base_url": "https://api.deepseek.com",
                            "model_name": "deepseek-chat",
                            "temperature": 0.3,
                            "max_tokens": 2048,
                            "timeout": 60
                        },
                        "gemini": {
                            "api_key": "",
                            "base_url": "https://generativelanguage.googleapis.com",
                            "model_name": "gemini-1.5-flash",
                            "temperature": 0.3,
                            "max_tokens": 2048,
                            "timeout": 60
                        }
                    }

                    with st.spinner(t["testing"]):
                        temp_service = LLMService(test_config)
                        success, message, models = asyncio.run(temp_service.test_connection())

                        if success:
                            st.success(t["test_success"])
                            # ä¿å­˜æµ‹è¯•æˆåŠŸçŠ¶æ€å’Œé…ç½®
                            st.session_state.first_setup_ollama_test_success = True
                            st.session_state.first_setup_ollama_test_config = test_config
                        else:
                            st.error(f"{t['test_failed']} {message}")
                            st.session_state.first_setup_ollama_test_success = False
                            st.session_state.first_setup_ollama_test_config = None

        # åœ¨è¡¨å•å¤–æ˜¾ç¤ºå®Œæˆè®¾ç½®æŒ‰é’®ï¼ˆåªæœ‰æµ‹è¯•æˆåŠŸåæ‰æ˜¾ç¤ºï¼‰
        if st.session_state.first_setup_ollama_test_success and st.session_state.first_setup_ollama_test_config:
            st.markdown("---")
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button(t["complete_setup"], type="primary", use_container_width=True, key="ollama_complete_setup"):
                    # è·å–é…ç½®å¹¶æ¸…ç† session state
                    config_to_return = st.session_state.first_setup_ollama_test_config
                    _cleanup_first_setup_state()
                    return config_to_return
    else:
        # æ²¡æœ‰è¡¨å•æ—¶ï¼Œæ˜¾ç¤ºè¿”å›æŒ‰é’®
        col1, col2, col3 = st.columns([1, 1, 1])
        with col2:
            if st.button(t["back"], key="first_setup_ollama_back", use_container_width=True):
                st.session_state.first_setup_step = 'provider_selection'
                st.rerun()

    return None

def _render_deepseek_setup(t: dict, language: str) -> Optional[Dict[str, Any]]:
    """æ¸²æŸ“ DeepSeek é…ç½®ç•Œé¢"""
    st.markdown(f"#### {t['deepseek_setup_title']}")

    # åˆå§‹åŒ–session stateæ¥ä¿å­˜æµ‹è¯•çŠ¶æ€
    if 'first_setup_deepseek_test_success' not in st.session_state:
        st.session_state.first_setup_deepseek_test_success = False
    if 'first_setup_deepseek_test_config' not in st.session_state:
        st.session_state.first_setup_deepseek_test_config = None

    with st.form("deepseek_setup_form"):
        api_key = st.text_input(
            t["api_key"],
            type="password",
            help=t["api_key_help"]
        )

        # å›ºå®šä½¿ç”¨ deepseek-chatï¼Œæ— éœ€ç”¨æˆ·é€‰æ‹©
        selected_model = "deepseek-chat"
        st.info("ğŸ’¡ Will use DeepSeek Chat model, suitable for various analysis tasks")

        col1, col2 = st.columns(2)
        with col1:
            back_clicked = st.form_submit_button(t["back"])
        with col2:
            test_clicked = st.form_submit_button(t["test_connection"], type="primary")

        if back_clicked:
            st.session_state.first_setup_step = 'provider_selection'
            # æ¸…ç†æµ‹è¯•çŠ¶æ€
            st.session_state.first_setup_deepseek_test_success = False
            st.session_state.first_setup_deepseek_test_config = None
            st.rerun()

        if test_clicked and api_key:
            with st.spinner(t["testing"]):
                # æµ‹è¯• DeepSeek è¿æ¥
                test_config = {
                    "provider": "deepseek",
                    "enabled": True,
                    "ollama": {
                        "base_url": "http://localhost:11434",
                        "model_name": "deepseek-r1:1.5b",
                        "temperature": 0.3,
                        "max_tokens": 2048,
                        "timeout": 60
                    },
                    "deepseek": {
                        "api_key": api_key,
                        "base_url": "https://api.deepseek.com",
                        "model_name": selected_model,
                        "temperature": 0.3,
                        "max_tokens": 2048,
                        "timeout": 60
                    },
                    "gemini": {
                        "api_key": "",
                        "base_url": "https://generativelanguage.googleapis.com",
                        "model_name": "gemini-1.5-flash",
                        "temperature": 0.3,
                        "max_tokens": 2048,
                        "timeout": 60
                    }
                }

                temp_service = LLMService(test_config)
                success, message, models = asyncio.run(temp_service.test_connection())

                if success:
                    st.success(t["test_success"])
                    # ä¿å­˜æµ‹è¯•æˆåŠŸçŠ¶æ€å’Œé…ç½®
                    st.session_state.first_setup_deepseek_test_success = True
                    st.session_state.first_setup_deepseek_test_config = test_config
                else:
                    st.error(f"{t['test_failed']} {message}")
                    st.session_state.first_setup_deepseek_test_success = False
                    st.session_state.first_setup_deepseek_test_config = None
        elif test_clicked and not api_key:
            st.error("Please enter API key")

    # åœ¨è¡¨å•å¤–æ˜¾ç¤ºå®Œæˆè®¾ç½®æŒ‰é’®ï¼ˆåªæœ‰æµ‹è¯•æˆåŠŸåæ‰æ˜¾ç¤ºï¼‰
    if st.session_state.first_setup_deepseek_test_success and st.session_state.first_setup_deepseek_test_config:
        st.markdown("---")
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button(t["complete_setup"], type="primary", use_container_width=True, key="deepseek_complete_setup"):
                # è·å–é…ç½®å¹¶æ¸…ç† session state
                config_to_return = st.session_state.first_setup_deepseek_test_config
                _cleanup_first_setup_state()
                return config_to_return

    return None

def _render_gemini_setup(t: dict, language: str) -> Optional[Dict[str, Any]]:
    """æ¸²æŸ“ Gemini é…ç½®ç•Œé¢"""
    st.markdown(f"#### {t['gemini_setup_title']}")

    # åˆå§‹åŒ– session state
    if 'first_setup_gemini_test_success' not in st.session_state:
        st.session_state.first_setup_gemini_test_success = False
    if 'first_setup_gemini_api_key' not in st.session_state:
        st.session_state.first_setup_gemini_api_key = ""
    if 'first_setup_gemini_model' not in st.session_state:
        st.session_state.first_setup_gemini_model = "gemini-1.5-flash"
    if 'first_setup_gemini_test_config' not in st.session_state:
        st.session_state.first_setup_gemini_test_config = None

    with st.form("gemini_setup_form"):
        api_key = st.text_input(
            t["api_key"],
            value=st.session_state.first_setup_gemini_api_key,
            type="password",
            help=t["gemini_api_key_help"]
        )
        # æ›´æ–° session state
        st.session_state.first_setup_gemini_api_key = api_key

        # Geminiæ¨¡å‹é€‰æ‹©
        gemini_models = [
            "gemini-1.5-flash",
            "gemini-1.5-pro",
            "gemini-1.0-pro"
        ]

        current_model_index = 0
        if st.session_state.first_setup_gemini_model in gemini_models:
            current_model_index = gemini_models.index(st.session_state.first_setup_gemini_model)

        selected_model = st.selectbox(
            t["model_selection"],
            options=gemini_models,
            index=current_model_index,
            help="Choose the Gemini model for analysis"
        )
        # æ›´æ–° session state
        st.session_state.first_setup_gemini_model = selected_model

        st.info("ğŸ’¡ Gemini-1.5-flash is recommended for fast response and good quality")

        col1, col2 = st.columns(2)
        with col1:
            back_clicked = st.form_submit_button(t["back"])
        with col2:
            test_clicked = st.form_submit_button(t["test_connection"], type="primary")

        if back_clicked:
            st.session_state.first_setup_step = 'provider_selection'
            st.rerun()

        if test_clicked and api_key:
            with st.spinner(t["testing"]):
                # æµ‹è¯• Gemini è¿æ¥
                test_config = {
                    "provider": "gemini",
                    "enabled": True,
                    "ollama": {
                        "base_url": "http://localhost:11434",
                        "model_name": "deepseek-r1:1.5b",
                        "temperature": 0.3,
                        "max_tokens": 2048,
                        "timeout": 60
                    },
                    "deepseek": {
                        "api_key": "",
                        "base_url": "https://api.deepseek.com",
                        "model_name": "deepseek-chat",
                        "temperature": 0.3,
                        "max_tokens": 2048,
                        "timeout": 60
                    },
                    "gemini": {
                        "api_key": api_key,
                        "base_url": "https://generativelanguage.googleapis.com",
                        "model_name": selected_model,
                        "temperature": 0.3,
                        "max_tokens": 2048,
                        "timeout": 60
                    }
                }

                temp_service = LLMService(test_config)
                success, message, models = asyncio.run(temp_service.test_connection())

                if success:
                    st.success(t["test_success"])
                    # è®¾ç½®æµ‹è¯•æˆåŠŸçŠ¶æ€
                    st.session_state.first_setup_gemini_test_success = True
                    st.session_state.first_setup_gemini_test_config = test_config
                else:
                    st.error(f"{t['test_failed']} {message}")
                    st.session_state.first_setup_gemini_test_success = False
        elif test_clicked and not api_key:
            st.error("Please enter API key")
            st.session_state.first_setup_gemini_test_success = False

    # åœ¨è¡¨å•å¤–æ˜¾ç¤ºå®Œæˆè®¾ç½®æŒ‰é’®ï¼ˆåªæœ‰æµ‹è¯•æˆåŠŸåæ‰æ˜¾ç¤ºï¼‰
    if st.session_state.first_setup_gemini_test_success and st.session_state.first_setup_gemini_test_config:
        st.markdown("---")
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button(t["complete_setup"], type="primary", use_container_width=True, key="gemini_complete_setup"):
                # è·å–é…ç½®å¹¶æ¸…ç† session state
                config_to_return = st.session_state.first_setup_gemini_test_config
                _cleanup_first_setup_state()
                return config_to_return

    return None

def _cleanup_first_setup_state():
    """æ¸…ç†é¦–æ¬¡è®¾ç½®çš„ session state"""
    keys_to_remove = [
        'first_setup_step',
        'first_setup_provider',
        'first_setup_ollama_base_url',
        'first_setup_ollama_models',
        'first_setup_ollama_models_error',
        'first_setup_ollama_manual_config',
        'first_setup_ollama_test_success',
        'first_setup_ollama_test_config',
        'first_setup_deepseek_test_success',
        'first_setup_deepseek_test_config',
        'first_setup_gemini_test_success',
        'first_setup_gemini_api_key',
        'first_setup_gemini_model',
        'first_setup_gemini_test_config'
    ]
    for key in keys_to_remove:
        try:
            if hasattr(st.session_state, key):
                delattr(st.session_state, key)
            elif hasattr(st.session_state, 'data') and key in st.session_state.data:
                del st.session_state.data[key]
        except (AttributeError, KeyError):
            pass  # å¿½ç•¥ä¸å­˜åœ¨çš„é”®

# ä¿æŒå‘åå…¼å®¹
def render_llm_quick_setup(language: str = "en") -> Optional[Dict[str, Any]]:
    """æ¸²æŸ“LLMå¿«é€Ÿè®¾ç½®ç•Œé¢ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    return render_llm_first_time_setup(language)
